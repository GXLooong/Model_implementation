{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75de1e12",
   "metadata": {},
   "source": [
    "# ä¸€ã€åŠŸèƒ½æ¢ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "209fa811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d3bc197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70bd38c6fa454d95923ae9323cbd4bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GXL\\.conda\\envs\\huggingface\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\GXL\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6221082029644baba85030db1a641a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f32f872ebf141b0b7f6778a61c91e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5810e65874449f8dffba044f38a5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74f47cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([7592, 1010, 2088, 999], 'hello, world!', ['hello', ',', 'world', '!'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# token_ids = tokenizer.encode(\"Hello, world!\")\n",
    "token_ids = tokenizer.encode(\"Hello, world!\", add_special_tokens=False)\n",
    "token_ids\n",
    "original_text = tokenizer.decode(token_ids)\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "token_ids, original_text, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aef3ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7592, 1010, 2088,  999,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1d9d8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ae97080",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['http_proxy'] = 'http://127.0.0.1:33210'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:33210'\n",
    "os.environ['all_proxy'] = 'socks5://127.0.0.1:33211'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6da096d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a049022e1cd547628773ff10b53b226b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ä½¿ç”¨osé…ç½®ä»£ç†å°±èƒ½æˆåŠŸï¼\n",
    "# ä¸çŸ¥é“åé¢ä¸‹è½½æ¨¡å‹å’Œä¸Šä¼ æ¨¡å‹ä¼šä¸ä¼šé¡ºåˆ©\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fdb18abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b583b021e240c79ec4deb430a409aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GXL\\.conda\\envs\\huggingface\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\GXL\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb722c80811d434cb1819cb5dc4b2386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226bfed62f4c4ba0b18b2f2d822fdcf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8b5a17d002476e82a0c7f49af3168c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# åŠ è½½tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2916887",
   "metadata": {},
   "source": [
    "## å¯¹tokenizeråŠŸèƒ½çš„æ¢ç´¢\n",
    "\n",
    "å…¶ä¸­æ¶‰åŠä¸‰ç§çŠ¶æ€ï¼š\n",
    "\n",
    "- æ–‡æœ¬text\n",
    "    - å­—ç¬¦ä¸²\n",
    "    - [å­—ç¬¦ä¸²1, å­—ç¬¦ä¸²2, .. å­—ç¬¦ä¸²n]\n",
    "- åˆ†æœ¬åˆ†è¯åçš„tokens\n",
    "    - [token1, token2, ... , token_n]\n",
    "    - å…¶ä¸­å¯èƒ½åŒ…å«å¡«å……çš„ç‰¹æ®Šæ ‡è®°[CLS][SEP]\n",
    "    - ä¹Ÿå¯èƒ½ä¸åŒ…æ‹¬ï¼ˆç›´æ¥ä½¿ç”¨encodeæ–¹æ³•ï¼‰\n",
    "- åˆ†è¯å¹¶è½¬åŒ–ä¸ºdigitçš„input_ids\n",
    "    - åªèƒ½æ˜¯å­—å…¸ï¼ŒåŒ…æ‹¬\"input_ids\",\"attention_mask\"è¿™äº›é”®ï¼Œå€¼æ˜¯list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc81703",
   "metadata": {},
   "source": [
    "<img src=\"..\\Apictures\\11.png\" alt=\"11\" style=\"zoom:85%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be246c7a",
   "metadata": {},
   "source": [
    "    - ä¸¤ä¸ªconvertæ–¹æ³•çš„è¾“å…¥å’Œè¾“å‡ºéƒ½æ˜¯list\n",
    "  \n",
    "    - é€šè¿‡encode_plus()å’Œ__call__æ–¹æ³•å¾—åˆ°çš„è¾“å‡ºæ˜¯å­—å…¸ï¼Œè€Œencode()å¾—åˆ°çš„è¾“å‡ºåªæ˜¯ä¸€ä¸ªdigitçš„listï¼Œ\n",
    "      ä½†æ˜¯ä¸‰è€…çš„digitä¸­éƒ½å¸¦æœ‰ç‰¹æ®Štokenå¯¹åº”çš„digit(é»˜è®¤å¸¦ï¼Œä¹Ÿå¯ä»¥é€šè¿‡è®¾ç½®add_special_tokens=Falseè®©å®ƒä¸å¸¦ï¼‰\n",
    "      \n",
    "    - tokenize()æ–¹æ³•äº§ç”Ÿçš„tokensä¸­ä¸å¸¦æœ‰ç‰¹æ®Šçš„token([CLS][SEP]è¿™äº›\n",
    "    \n",
    "    - å¯¹äºlist(str)çš„è¾“å…¥ï¼š\n",
    "        __call__æ–¹æ³•äº§ç”Ÿçš„å­—å…¸ä¸­\"input_ids\"å¯¹åº”çš„digitæ˜¯å¤šç»´çš„ï¼Œç›¸å½“äºå¤šä¸ªå¥å­åˆ†åˆ«ç¼–ç \n",
    "        encode()å’Œencode_plus()åªèƒ½æ¥æ”¶listä¸­æœ‰ä¸¤ä¸ªstr,è¡¨ç¤ºä¸€ä¸ªå¥å­å¯¹ï¼Œäº§ç”Ÿçš„ç»“æœæ˜¯è®²è¿™ä¸¤ä¸ªå¥å­å¯¹æ‹¼æ¥æˆä¸€ä¸ªå¥å­ï¼Œè€Œä¸æ˜¯ä¸¤ä¸ªå¥å­å•ç‹¬å¤„ç†\n",
    "        ä¸¤ä¸ªencodeæ–¹æ³•åœ¨è¾“å…¥ä¸Šä¹Ÿæœ‰ä¸€äº›å¾®å°çš„å·®åˆ«ï¼Œencode_plus()è¿˜å¯ä»¥æ¥æ”¶ä¸¤ä¸ªsentenceçš„strä¼ å…¥ä½œä¸ºå¥å­å¯¹ï¼Œ\n",
    "        è€Œencodeåªèƒ½æ¥æ”¶list(str)ä½œä¸ºå¥å­å¯¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c2e3a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#text = \"Hello, Hugging Face!\"\n",
    "# text2 = \"oh yeah!\"\n",
    "# text = [\"Hello, Hugging Face!\", \"Transformers are amazing.\", \"good exampleï¼\"]\n",
    "text = [\"Hello, Hugging Face!\", \"Transformers are amazing.\"]\n",
    "\n",
    "\n",
    "# æ–‡æœ¬ â†’ inputï¼ˆå…¶ä¸­è¿˜åŒ…æ‹¬äº†attention_maskï¼‰\n",
    "# åº•å±‚è°ƒç”¨çš„æ˜¯__call__æ–¹æ³•\n",
    "encoded_input = tokenizer(text)\n",
    "print(\"encoded_input:\\n\" + str(encoded_input))\n",
    "\n",
    "# encodeä¸__call__è¿›è¡Œå¯¹æ¯”\n",
    "encode_input = tokenizer.encode(text)\n",
    "print(\"encode_input:\\n\" + str(encode_input))\n",
    "# encode_plusä¸__call__å¯¹æ¯”\n",
    "encode_plus_input = tokenizer.encode_plus(text)\n",
    "print(\"encode_plus_input:\\n\" + str(encode_plus_input))\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"tokens:\\n\" + str(tokens))\n",
    "\n",
    "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"tokens_ids:\\n\" + str(tokens_ids))\n",
    "tokens_ids = tokenizer(tokens, is_split_into_words=True)\n",
    "print(\"tokens_ids:\\n\" + str(tokens_ids))\n",
    "\n",
    "tokens_reverse = tokenizer.convert_ids_to_tokens(encode_input)\n",
    "tokens_reverse, tokenizer.convert_tokens_to_ids(tokens_reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c837a",
   "metadata": {},
   "source": [
    "## æ•°æ®é›†é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b9103d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea3531935ea4e028122893795e2e708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0859788f1d34030ab8055d6131e6559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/4.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ee9c0dc8ae49e08ad89f083def2bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/9.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ec05b6537847c9b3dcab0643b36b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9046328cc394c97b5873f1657e86fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05524d6f17ff45d88160e78fb3c0474b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/39.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b760ecb2f74a4d4caf604e6542915faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/66.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8406fc1702184ada9aa0fd4f846bd0ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec55390e9e245acab747ebed684a4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ec86a57b70450e80f9cedf2042afd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd078a07be3641df875cb2e85a5551c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wnut = load_dataset(\"wnut_17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "4000b6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 3394\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1009\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1287\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a9c5ecfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'tokens', 'ner_tags'],\n",
       "     num_rows: 3394\n",
       " }),\n",
       " {'id': Value(dtype='string', id=None),\n",
       "  'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       "  'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-corporation', 'I-corporation', 'B-creative-work', 'I-creative-work', 'B-group', 'I-group', 'B-location', 'I-location', 'B-person', 'I-person', 'B-product', 'I-product'], id=None), length=-1, id=None)},\n",
       " datasets.features.features.Features)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut['train'], wnut[\"train\"].features, type(wnut[\"train\"].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1c5a8536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "599c1e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79cf2210cac4223a08cf38eeabc7dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a437bded4d64ccb9d5addf104378864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754def60861e4eafb88f1190f2866df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a9fcb2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3394\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1009\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1287\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_wnut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "288aaeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c2d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7d221305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_inputs:\n",
      "{'input_ids': [[101, 2129, 2024, 2017, 1029, 1029, 102], [101, 1045, 1005, 1049, 2200, 2200, 23292, 2595, 6834, 2986, 4283, 999, 1998, 2017, 1029, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method BatchEncoding.word_ids of {'input_ids': [[101, 2129, 2024, 2017, 1029, 1029, 102], [101, 1045, 1005, 1049, 2200, 2200, 23292, 2595, 6834, 2986, 4283, 999, 1998, 2017, 1029, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [[\"how\", \"are\", \"you?\", \"?\"], [\"I'm very very fxxking\", \"fine\", \"thanks\", \"!\", \"and\", \"you?\"]]\n",
    "# texts = [\"how are you?\", \"I am fine, and you?\"]\n",
    "\n",
    "tokenized_inputs = tokenizer(texts, truncation=True, is_split_into_words=True)\n",
    "print(\"tokenized_inputs:\\n\" + str(tokenized_inputs))\n",
    "tokenized_inputs.word_ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0ca11e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples:\n",
      "{'id': ['0', '1', '2'], 'tokens': [['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.'], ['From', 'Green', 'Newsfeed', ':', 'AHFA', 'extends', 'deadline', 'for', 'Sage', 'Award', 'to', 'Nov', '.', '5', 'http://tinyurl.com/24agj38'], ['Pxleyes', 'Top', '50', 'Photography', 'Contest', 'Pictures', 'of', 'August', '2010', '...', 'http://bit.ly/bgCyZ0', '#photography']], 'ner_tags': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "\n",
      "tokenized_inputs:\n",
      "{'input_ids': [[101, 1030, 2703, 17122, 2009, 1005, 1055, 1996, 3193, 2013, 2073, 1045, 1005, 1049, 2542, 2005, 2048, 3134, 1012, 3400, 2110, 2311, 1027, 9686, 2497, 1012, 3492, 2919, 4040, 2182, 2197, 3944, 1012, 102], [101, 2013, 2665, 2739, 7959, 2098, 1024, 6289, 7011, 8908, 15117, 2005, 10878, 2400, 2000, 13292, 1012, 1019, 8299, 1024, 1013, 1013, 4714, 3126, 2140, 1012, 4012, 1013, 2484, 8490, 3501, 22025, 102], [101, 1052, 20959, 2229, 2327, 2753, 5855, 5049, 4620, 1997, 2257, 2230, 1012, 1012, 1012, 8299, 1024, 1013, 1013, 2978, 1012, 1048, 2100, 1013, 1038, 18195, 2100, 2480, 2692, 1001, 5855, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "\n",
      "[None, 0, 1, 2, 2, 2, 3, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, None]\n"
     ]
    }
   ],
   "source": [
    "examples = wnut['train'][:3]\n",
    "print(\"examples:\\n\" + str(examples) + \"\\n\")\n",
    "\n",
    "tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "print(\"tokenized_inputs:\\n\" + str(tokenized_inputs) + '\\n')\n",
    "\n",
    "# word_idsæ˜¯ä¸€ä¸ªæ–¹æ³•ï¼Œä¸æŒ‡å®šbatch_indexè‡³å°‘åŠ ä¸Šæ‹¬å·å•Šï¼ï¼ˆä¸æŒ‡å®šé»˜è®¤æ˜¯0ï¼‰\n",
    "print(tokenized_inputs.word_ids(batch_index=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "44c7e553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example:\n",
      "{'id': '0', 'tokens': ['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1030, 2703, 17122, 2009, 1005, 1055, 1996, 3193, 2013, 2073, 1045, 1005, 1049, 2542, 2005, 2048, 3134, 1012, 3400, 2110, 2311, 1027, 9686, 2497, 1012, 3492, 2919, 4040, 2182, 2197, 3944, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¿›ä¸€æ­¥åˆ†è¯ä¸ºsubwords\n",
    "\n",
    "example = wnut[\"train\"][0]\n",
    "print(\"example:\\n\" + str(example) + \"\\n\")\n",
    "\n",
    "tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30781467",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"O\", \"B-BIAS\", \"I-BIAS\"] # æ ¹æ®éœ€è¦æ·»åŠ å…¶ä»–æ ‡ç­¾\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4458ff",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf22e101",
   "metadata": {},
   "source": [
    "# äºŒã€è®­ç»ƒBias-NERæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d0cf63",
   "metadata": {},
   "source": [
    "## 1.æ•°æ®é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e768cec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GXL\\.conda\\envs\\huggingface\\lib\\site-packages\\huggingface_hub\\repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½æ•°æ®\n",
    "bias_conll = load_dataset(\"newsmediabias/BIAS-CONLL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f677eced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['Unnamed: 0', 'sentence_id', 'words', 'labels'],\n",
       "         num_rows: 50575\n",
       "     })\n",
       " }),\n",
       " {'Unnamed: 0': [0, 1, 2],\n",
       "  'sentence_id': [0, 0, 0],\n",
       "  'words': ['rachel_scanlan', 'thanks', 'for'],\n",
       "  'labels': ['O', 'O', 'O']})"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_conll, bias_conll[\"train\"][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "107799f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¬¬0ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬1000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬2000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬3000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬4000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬5000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬6000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬7000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬8000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬9000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬10000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬11000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬12000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬13000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬14000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬15000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬16000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬17000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬18000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬19000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬20000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬21000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬22000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬23000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬24000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬25000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬26000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬27000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬28000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬29000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬30000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬31000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬32000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬33000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬34000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬35000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬36000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬37000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬38000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬39000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬40000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬41000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬42000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬43000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬44000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬45000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬46000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬47000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬48000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬49000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n",
      "ç¬¬50000ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[160], line 34\u001b[0m\n\u001b[0;32m     30\u001b[0m grouped_train_dataset \u001b[38;5;241m=\u001b[39m group_words_labels(bias_conll[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# grouped_data = bias_conll[\"train\"].map(group_words_labels, batched=True, batch_size=25)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# å°†èšåˆåçš„æ•°æ®è½¬æ¢ä¸º Dataset ç±»å‹\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m grouped_train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_dict({\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m: grouped_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: grouped_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     37\u001b[0m })\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# æ˜¾ç¤ºç»“æœ\u001b[39;00m\n\u001b[0;32m     40\u001b[0m grouped_train_dataset, grouped_train_dataset[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# å®šä¹‰åˆ†ç»„å’Œèšåˆå‡½æ•°\n",
    "def group_words_labels(examples):\n",
    "    # åˆ†ç»„æ“ä½œåœ¨è¿™é‡Œæ˜¯é€ä¸ªæ‰§è¡Œçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å…ˆå°†å®ƒä»¬ç´¯ç§¯åœ¨åˆ—è¡¨ä¸­\n",
    "    grouped_words = []\n",
    "    grouped_labels = []\n",
    "    previous_id = None\n",
    "    words = []\n",
    "    labels = []\n",
    "    for i in range(len(examples['sentence_id'])):\n",
    "        if previous_id is None or previous_id == examples['sentence_id'][i]:\n",
    "            # å¦‚æœæ˜¯åŒä¸€å¥å­ï¼Œç´¯ç§¯å•è¯å’Œæ ‡ç­¾\n",
    "            words.append(examples['words'][i])\n",
    "            labels.append(examples['labels'][i])\n",
    "        else:\n",
    "            # å¦‚æœæ˜¯æ–°å¥å­ï¼Œä¿å­˜å‰ä¸€å¥å­çš„ç´¯ç§¯ç»“æœï¼Œå¹¶é‡æ–°å¼€å§‹ç´¯ç§¯\n",
    "            grouped_words.append(words)\n",
    "            grouped_labels.append(labels)\n",
    "            words = [examples['words'][i]]\n",
    "            labels = [examples['labels'][i]]\n",
    "        previous_id = examples['sentence_id'][i]\n",
    "        if(i % 1000 == 0):\n",
    "            print(\"ç¬¬\" + str(i) + \"ä¸ªæ ·æœ¬å·²ç»å¤„ç†å®Œ\")\n",
    "    # ç¡®ä¿æœ€åä¸€å¥è¯ä¹Ÿè¢«æ·»åŠ \n",
    "    grouped_words.append(words)\n",
    "    grouped_labels.append(labels)\n",
    "    # è¿”å›åˆ†ç»„ç»“æœ\n",
    "    return {'words': grouped_words, 'labels': grouped_labels}\n",
    "\n",
    "# å¯¹è®­ç»ƒé›†åº”ç”¨åˆ†ç»„å‡½æ•°\n",
    "grouped_train_dataset = group_words_labels(bias_conll[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "71f8d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ˜ å°„\n",
    "\n",
    "label_list = ['O', 'B-BIAS', 'I-BIAS'] # æ ¹æ®éœ€è¦æ·»åŠ å…¶ä»–æ ‡ç­¾\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4bed59fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ba40ed266e44b1ac932fb8d8289a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': [['rachel_scanlan', 'thanks', 'for', 'following', 'your', 'website', 'looks', 'like', 'greek', 'to', 'me', 'and', 'i', 'couldn', 't', 'see', 'your', 'tweets'], ['Waiting', 'for', 'it', 'to', 'rain', 'for', 'our', 'golf', 'tournament', 'today']], 'labels': [[0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 2, 0, 0, 1, 0, 0, 0, 0], [1, 2, 0, 0, 0, 0, 0, 1, 2, 0]]}\n"
     ]
    }
   ],
   "source": [
    "# å°†labelsä»ç±»æ ‡ç­¾è½¬åŒ–ä¸ºdigits\n",
    "\n",
    "# è½¬æ¢æ ‡ç­¾åˆ° ID çš„å‡½æ•°\n",
    "def labels_to_ids(examples):\n",
    "    # ä½¿ç”¨æ ‡ç­¾åˆ° ID çš„æ˜ å°„æ¥è½¬æ¢æ¯ä¸ªæ ‡ç­¾\n",
    "    try:\n",
    "        examples['labels'] = [[label_to_id[label] for label in sentence_labels] for sentence_labels in examples['labels']]\n",
    "    except KeyError as e:\n",
    "        print(f\"æ‰¾ä¸åˆ°æ ‡ç­¾: {e}\")\n",
    "    return examples\n",
    "\n",
    "# åœ¨ grouped_train_dataset ä¸Šåº”ç”¨è¿™ä¸ªå‡½æ•°\n",
    "grouped_train_dataset = grouped_train_dataset.map(labels_to_ids, batched=True)\n",
    "\n",
    "# æŸ¥çœ‹è½¬æ¢åçš„å‰ä¸¤ä¸ªæ ·æœ¬\n",
    "print(grouped_train_dataset[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "aa1207e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['words', 'labels'],\n",
       "     num_rows: 2715\n",
       " }),\n",
       " {'words': [['rachel_scanlan',\n",
       "    'thanks',\n",
       "    'for',\n",
       "    'following',\n",
       "    'your',\n",
       "    'website',\n",
       "    'looks',\n",
       "    'like',\n",
       "    'greek',\n",
       "    'to',\n",
       "    'me',\n",
       "    'and',\n",
       "    'i',\n",
       "    'couldn',\n",
       "    't',\n",
       "    'see',\n",
       "    'your',\n",
       "    'tweets'],\n",
       "   ['Waiting',\n",
       "    'for',\n",
       "    'it',\n",
       "    'to',\n",
       "    'rain',\n",
       "    'for',\n",
       "    'our',\n",
       "    'golf',\n",
       "    'tournament',\n",
       "    'today']],\n",
       "  'labels': [[0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 2, 0, 0, 1, 0, 0, 0, 0],\n",
       "   [1, 2, 0, 0, 0, 0, 0, 1, 2, 0]]})"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# å°†èšåˆåçš„æ•°æ®è½¬æ¢ä¸º Dataset ç±»å‹\n",
    "grouped_train_dataset = Dataset.from_dict({\n",
    "    'words': grouped_train_dataset['words'],\n",
    "    'labels': grouped_train_dataset['labels']\n",
    "})\n",
    "\n",
    "# æ˜¾ç¤ºç»“æœ\n",
    "grouped_train_dataset, grouped_train_dataset[0: 2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d67638f",
   "metadata": {},
   "source": [
    "## 2.åŠ è½½tokenizer,è¿›ä¸€æ­¥å¤„ç†æ•°æ®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e2ed27",
   "metadata": {},
   "source": [
    "### 2.1 åŠ è½½tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "28da0073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰é¢å·²ç»åŠ è½½è¿‡äº†ï¼Œæ‰€ä»¥è¿™é‡Œåªæ˜¯ç²˜è´´ä¸€ä¸‹ä»£ç \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed20d62b",
   "metadata": {},
   "source": [
    "### 2.2 æ·»åŠ idåˆ—  & ä¿®æ”¹wordsåˆ—å’Œlabelsåˆ—åå­—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "b4a20994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['tokens', 'ner_tags', 'id'],\n",
       "     num_rows: 2715\n",
       " }),\n",
       " {'tokens': [['rachel_scanlan',\n",
       "    'thanks',\n",
       "    'for',\n",
       "    'following',\n",
       "    'your',\n",
       "    'website',\n",
       "    'looks',\n",
       "    'like',\n",
       "    'greek',\n",
       "    'to',\n",
       "    'me',\n",
       "    'and',\n",
       "    'i',\n",
       "    'couldn',\n",
       "    't',\n",
       "    'see',\n",
       "    'your',\n",
       "    'tweets'],\n",
       "   ['Waiting',\n",
       "    'for',\n",
       "    'it',\n",
       "    'to',\n",
       "    'rain',\n",
       "    'for',\n",
       "    'our',\n",
       "    'golf',\n",
       "    'tournament',\n",
       "    'today']],\n",
       "  'ner_tags': [[0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 2, 0, 0, 1, 0, 0, 0, 0],\n",
       "   [1, 2, 0, 0, 0, 0, 0, 1, 2, 0]],\n",
       "  'id': ['0', '1']})"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_list = [str(i) for i in range(len(grouped_train_dataset))]\n",
    "\n",
    "# å¿…é¡»æ·»åŠ idåˆ—\n",
    "# å› ä¸ºmap()æ–¹æ³•è¿›è¡Œåˆ†æ‰¹æ—¶ï¼Œéœ€è¦ç”¨åˆ°id,å¦‚æœæ²¡æœ‰å°±ä¼šæŠ¥é”™\n",
    "# grouped_train_dataset = grouped_train_dataset.add_column(\"id\", id_list)\n",
    "\n",
    "\n",
    "# ç»™wordså’Œlabelsåˆ—æ”¹å\n",
    "# grouped_train_dataset = grouped_train_dataset.rename_column(\"labels\", \"ner_tags\")\n",
    "# grouped_train_dataset = grouped_train_dataset.rename_column(\"words\", \"tokens\")\n",
    "grouped_train_dataset, grouped_train_dataset[0:2] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f016e785",
   "metadata": {},
   "source": [
    "### è½¬æ¢ner-tagsçš„æ•°æ®ç±»å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "184462e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9137842bd6745208f5f0abf3273c28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'id'],\n",
       "    num_rows: 2715\n",
       "})"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import ClassLabel, Sequence\n",
    "class_label = ClassLabel(names=['O', 'B-BIAS', 'I-BIAS'])\n",
    "\n",
    "new_features = grouped_train_dataset.features.copy()\n",
    "sequence_class_label = Sequence(feature=class_label, length=-1)\n",
    "new_features['ner_tags'] = sequence_class_label\n",
    "grouped_train_dataset_ = grouped_train_dataset.cast(new_features)\n",
    "grouped_train_dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "c5f8595b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-BIAS', 'I-BIAS'], id=None), length=-1, id=None),\n",
       " 'id': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_train_dataset_.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f514b62d",
   "metadata": {},
   "source": [
    "### äº¤æ¢keyçš„é¡ºåº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "f9dbf8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138e0036c2224f2eb09692d87e5623e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def reorder_features(example):\n",
    "    return {'id': example['id'], 'tokens': example['tokens'], 'ner_tags': example['ner_tags']}\n",
    "\n",
    "# ä½¿ç”¨ map å‡½æ•°åº”ç”¨è¿™ä¸ªè½¬æ¢ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®é›†\n",
    "reordered_dataset = grouped_train_dataset_.map(reorder_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "549a7b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf1e54e2fd24607a86e062a6d57f93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-BIAS', 'I-BIAS'], id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, Features, Value, Sequence, ClassLabel\n",
    "\n",
    "# è·å–åŸå§‹æ•°æ®é›†çš„ç‰¹å¾ï¼Œå¹¶æŒ‰æ–°çš„é¡ºåºé‡æ–°æ’åˆ—\n",
    "original_features = grouped_train_dataset_.features\n",
    "new_order_features = Features({\n",
    "    'id': original_features['id'],\n",
    "    'tokens': original_features['tokens'],\n",
    "    'ner_tags': original_features['ner_tags']\n",
    "})\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œå…¶ä¸­åˆ—çš„é¡ºåºæ˜¯æŒ‰ç…§æ–°çš„ç‰¹å¾é¡ºåºæ’åˆ—çš„\n",
    "reordered_dataset = grouped_train_dataset_.cast(new_order_features)\n",
    "\n",
    "# æ£€æŸ¥æ–°æ•°æ®é›†çš„ç‰¹å¾ä»¥ç¡®è®¤é¡ºåº\n",
    "print(reordered_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "4f1600dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'id': Value(dtype='string', id=None),\n",
       "  'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       "  'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-BIAS', 'I-BIAS'], id=None), length=-1, id=None)},\n",
       " {'id': Value(dtype='string', id=None),\n",
       "  'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       "  'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-corporation', 'I-corporation', 'B-creative-work', 'I-creative-work', 'B-group', 'I-group', 'B-location', 'I-location', 'B-person', 'I-person', 'B-product', 'I-product'], id=None), length=-1, id=None)})"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordered_dataset.features, wnut[\"validation\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a4a3b",
   "metadata": {},
   "source": [
    "### 2.3 è¿›ä¸€æ­¥åˆ†è¯å’Œlabelså¯¹é½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "4d7d6ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "0d950ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2715"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reordered_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb5230e",
   "metadata": {},
   "source": [
    "### ğŸš©æ¸…é™¤åæ•°æ®ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "572be1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '314',\n",
       " 'tokens': ['Only',\n",
       "  '3',\n",
       "  'concerts',\n",
       "  'quot',\n",
       "  'HITY',\n",
       "  None,\n",
       "  'CZASIE',\n",
       "  'quot',\n",
       "  'in',\n",
       "  'these',\n",
       "  'summer',\n",
       "  'holidays'],\n",
       " 'ner_tags': [1, 2, 1, 0, 1, 2, 2, 0, 0, 0, 1, 2]}"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordered_dataset[314]\n",
    "# åŒ…å«noneå…ƒç´ ï¼Œä¼šå¯¼è‡´ä½¿ç”¨tokenize_and_align_labelsæ–¹å¼ä¸­çš„tokenizerçš„æ—¶å€™æŠ¥é”™ï¼š\n",
    "# TypeError: PreTokenizedEncodeInput must be Union[PreTokenizedInputSequence, \n",
    "# Tuple[PreTokenizedInputSequence, PreTokenizedInputSequence]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "95b028cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a5712529e24fe99a216bf5b166a676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def remove_none_values(example):\n",
    "    # è·å–tokenså’Œner_tagsåˆ—è¡¨\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "    \n",
    "    # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼ç§»é™¤Noneå€¼åŠå…¶å¯¹åº”çš„ner_tags\n",
    "    new_tokens, new_ner_tags = zip(*[(token, tag) for token, tag in zip(tokens, ner_tags) if token is not None])\n",
    "    \n",
    "    # æ›´æ–°example\n",
    "    example['tokens'] = list(new_tokens)\n",
    "    example['ner_tags'] = list(new_ner_tags)\n",
    "    return example\n",
    "\n",
    "# åº”ç”¨è¿™ä¸ªå‡½æ•°åˆ°æ•´ä¸ªæ•°æ®é›†\n",
    "cleaned_dataset = reordered_dataset.map(remove_none_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "ca2d8dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'ner_tags'],\n",
       "    num_rows: 2715\n",
       "})"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "18f24c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7df85abf444f1d9452ef37f9be4067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_dataset = cleaned_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "ac64c0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2715\n",
       "})"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae394f09",
   "metadata": {},
   "source": [
    "### åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "effc56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = final_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# è·å–è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "e8ca975f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 2172\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 543\n",
       " }))"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d624175b",
   "metadata": {},
   "source": [
    "### ğŸš©ä¸Šä¼ æ•°æ®é›†åˆ°hugging faceåº“ä¸­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "b52bcb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0afa05702a4e0eb60b7407a11dc1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4832a0c244640d6a7b041be1b1e902b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7fa8253bace4746946d7e2e68b208f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c415f3226b46fd944e42f18ce7dfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46bf6e734307462485e524c58586e18a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "# å‡è®¾ cleaned_dataset æ˜¯æ‚¨è¦ä¸Šä¼ çš„æ•°æ®é›†\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset ,\n",
    "    \"validation\": val_dataset\n",
    "})\n",
    "\n",
    "# æ¨é€æ•°æ®é›†åˆ°æ‚¨çš„ Hugging Face å­˜å‚¨åº“\n",
    "dataset_dict.push_to_hub(\"GXLooong/cleaned_BIAS_CONLL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1655c8b",
   "metadata": {},
   "source": [
    "## 3.åŠ è½½model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "5370286d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1bbfdde129147f2bedb373511c57cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=3, id2label=id_to_label, label2id=label_to_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11e4dd0",
   "metadata": {},
   "source": [
    "## 4. è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4b514c",
   "metadata": {},
   "source": [
    "### 4.1 è¯„ä»·æŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "bde47384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "e7e78200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "acda18d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[392], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbias_detection_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     16\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     24\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m<string>:112\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, xpu_backend)\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\huggingface\\lib\\site-packages\\transformers\\training_args.py:1372\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(version\u001b[38;5;241m.\u001b[39mparse(torch\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mbase_version) \u001b[38;5;241m==\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16:\n\u001b[0;32m   1367\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m-> 1372\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1373\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (get_xla_device_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[0;32m   1375\u001b[0m ):\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1377\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m     )\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1388\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[0;32m   1389\u001b[0m ):\n",
      "File \u001b[1;32m~\\.conda\\envs\\huggingface\\lib\\site-packages\\transformers\\training_args.py:1795\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1794\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 1795\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\huggingface\\lib\\site-packages\\transformers\\utils\\generic.py:54\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     52\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32m~\\.conda\\envs\\huggingface\\lib\\site-packages\\transformers\\training_args.py:1716\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   1715\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available(min_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.20.1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1716\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   1717\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1718\u001b[0m         )\n\u001b[0;32m   1719\u001b[0m     AcceleratorState\u001b[38;5;241m.\u001b[39m_reset_state(reset_partial_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bias_detection_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe89d901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75de1e12",
   "metadata": {},
   "source": [
    "# 一、功能探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "209fa811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d3bc197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70bd38c6fa454d95923ae9323cbd4bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GXL\\.conda\\envs\\huggingface\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\GXL\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6221082029644baba85030db1a641a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f32f872ebf141b0b7f6778a61c91e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5810e65874449f8dffba044f38a5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74f47cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([7592, 1010, 2088, 999], 'hello, world!', ['hello', ',', 'world', '!'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# token_ids = tokenizer.encode(\"Hello, world!\")\n",
    "token_ids = tokenizer.encode(\"Hello, world!\", add_special_tokens=False)\n",
    "token_ids\n",
    "original_text = tokenizer.decode(token_ids)\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "token_ids, original_text, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aef3ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7592, 1010, 2088,  999,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1d9d8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ae97080",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['http_proxy'] = 'http://127.0.0.1:33210'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:33210'\n",
    "os.environ['all_proxy'] = 'socks5://127.0.0.1:33211'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6da096d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a049022e1cd547628773ff10b53b226b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 使用os配置代理就能成功！\n",
    "# 不知道后面下载模型和上传模型会不会顺利\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fdb18abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b583b021e240c79ec4deb430a409aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GXL\\.conda\\envs\\huggingface\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\GXL\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb722c80811d434cb1819cb5dc4b2386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226bfed62f4c4ba0b18b2f2d822fdcf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8b5a17d002476e82a0c7f49af3168c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2916887",
   "metadata": {},
   "source": [
    "## 对tokenizer功能的探索\n",
    "\n",
    "其中涉及三种状态：\n",
    "\n",
    "- 文本text\n",
    "    - 字符串\n",
    "    - [字符串1, 字符串2, .. 字符串n]\n",
    "- 分本分词后的tokens\n",
    "    - [token1, token2, ... , token_n]\n",
    "    - 其中可能包含填充的特殊标记[CLS][SEP]\n",
    "    - 也可能不包括（直接使用encode方法）\n",
    "- 分词并转化为digit的input_ids\n",
    "    - 只能是字典，包括\"input_ids\",\"attention_mask\"这些键，值是list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc81703",
   "metadata": {},
   "source": [
    "<img src=\"..\\Apictures\\11.png\" alt=\"11\" style=\"zoom:85%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be246c7a",
   "metadata": {},
   "source": [
    "    - 两个convert方法的输入和输出都是list\n",
    "  \n",
    "    - 通过encode_plus()和__call__方法得到的输出是字典，而encode()得到的输出只是一个digit的list，\n",
    "      但是三者的digit中都带有特殊token对应的digit(默认带，也可以通过设置add_special_tokens=False让它不带）\n",
    "      \n",
    "    - tokenize()方法产生的tokens中不带有特殊的token([CLS][SEP]这些\n",
    "    \n",
    "    - 对于list(str)的输入：\n",
    "        __call__方法产生的字典中\"input_ids\"对应的digit是多维的，相当于多个句子分别编码\n",
    "        encode()和encode_plus()只能接收list中有两个str,表示一个句子对，产生的结果是讲这两个句子对拼接成一个句子，而不是两个句子单独处理\n",
    "        两个encode方法在输入上也有一些微小的差别，encode_plus()还可以接收两个sentence的str传入作为句子对，\n",
    "        而encode只能接收list(str)作为句子对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c2e3a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#text = \"Hello, Hugging Face!\"\n",
    "# text2 = \"oh yeah!\"\n",
    "# text = [\"Hello, Hugging Face!\", \"Transformers are amazing.\", \"good example！\"]\n",
    "text = [\"Hello, Hugging Face!\", \"Transformers are amazing.\"]\n",
    "\n",
    "\n",
    "# 文本 → input（其中还包括了attention_mask）\n",
    "# 底层调用的是__call__方法\n",
    "encoded_input = tokenizer(text)\n",
    "print(\"encoded_input:\\n\" + str(encoded_input))\n",
    "\n",
    "# encode与__call__进行对比\n",
    "encode_input = tokenizer.encode(text)\n",
    "print(\"encode_input:\\n\" + str(encode_input))\n",
    "# encode_plus与__call__对比\n",
    "encode_plus_input = tokenizer.encode_plus(text)\n",
    "print(\"encode_plus_input:\\n\" + str(encode_plus_input))\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"tokens:\\n\" + str(tokens))\n",
    "\n",
    "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"tokens_ids:\\n\" + str(tokens_ids))\n",
    "tokens_ids = tokenizer(tokens, is_split_into_words=True)\n",
    "print(\"tokens_ids:\\n\" + str(tokens_ids))\n",
    "\n",
    "tokens_reverse = tokenizer.convert_ids_to_tokens(encode_input)\n",
    "tokens_reverse, tokenizer.convert_tokens_to_ids(tokens_reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c837a",
   "metadata": {},
   "source": [
    "## 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b9103d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea3531935ea4e028122893795e2e708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0859788f1d34030ab8055d6131e6559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/4.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ee9c0dc8ae49e08ad89f083def2bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/9.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ec05b6537847c9b3dcab0643b36b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9046328cc394c97b5873f1657e86fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05524d6f17ff45d88160e78fb3c0474b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/39.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b760ecb2f74a4d4caf604e6542915faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/66.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8406fc1702184ada9aa0fd4f846bd0ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec55390e9e245acab747ebed684a4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ec86a57b70450e80f9cedf2042afd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd078a07be3641df875cb2e85a5551c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wnut = load_dataset(\"wnut_17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "4000b6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 3394\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1009\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1287\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a9c5ecfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'tokens', 'ner_tags'],\n",
       "     num_rows: 3394\n",
       " }),\n",
       " {'id': Value(dtype='string', id=None),\n",
       "  'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       "  'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-corporation', 'I-corporation', 'B-creative-work', 'I-creative-work', 'B-group', 'I-group', 'B-location', 'I-location', 'B-person', 'I-person', 'B-product', 'I-product'], id=None), length=-1, id=None)},\n",
       " datasets.features.features.Features)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut['train'], wnut[\"train\"].features, type(wnut[\"train\"].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1c5a8536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "599c1e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79cf2210cac4223a08cf38eeabc7dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a437bded4d64ccb9d5addf104378864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754def60861e4eafb88f1190f2866df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a9fcb2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3394\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1009\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1287\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_wnut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "288aaeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c2d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7d221305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_inputs:\n",
      "{'input_ids': [[101, 2129, 2024, 2017, 1029, 1029, 102], [101, 1045, 1005, 1049, 2200, 2200, 23292, 2595, 6834, 2986, 4283, 999, 1998, 2017, 1029, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method BatchEncoding.word_ids of {'input_ids': [[101, 2129, 2024, 2017, 1029, 1029, 102], [101, 1045, 1005, 1049, 2200, 2200, 23292, 2595, 6834, 2986, 4283, 999, 1998, 2017, 1029, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [[\"how\", \"are\", \"you?\", \"?\"], [\"I'm very very fxxking\", \"fine\", \"thanks\", \"!\", \"and\", \"you?\"]]\n",
    "# texts = [\"how are you?\", \"I am fine, and you?\"]\n",
    "\n",
    "tokenized_inputs = tokenizer(texts, truncation=True, is_split_into_words=True)\n",
    "print(\"tokenized_inputs:\\n\" + str(tokenized_inputs))\n",
    "tokenized_inputs.word_ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0ca11e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples:\n",
      "{'id': ['0', '1', '2'], 'tokens': [['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.'], ['From', 'Green', 'Newsfeed', ':', 'AHFA', 'extends', 'deadline', 'for', 'Sage', 'Award', 'to', 'Nov', '.', '5', 'http://tinyurl.com/24agj38'], ['Pxleyes', 'Top', '50', 'Photography', 'Contest', 'Pictures', 'of', 'August', '2010', '...', 'http://bit.ly/bgCyZ0', '#photography']], 'ner_tags': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "\n",
      "tokenized_inputs:\n",
      "{'input_ids': [[101, 1030, 2703, 17122, 2009, 1005, 1055, 1996, 3193, 2013, 2073, 1045, 1005, 1049, 2542, 2005, 2048, 3134, 1012, 3400, 2110, 2311, 1027, 9686, 2497, 1012, 3492, 2919, 4040, 2182, 2197, 3944, 1012, 102], [101, 2013, 2665, 2739, 7959, 2098, 1024, 6289, 7011, 8908, 15117, 2005, 10878, 2400, 2000, 13292, 1012, 1019, 8299, 1024, 1013, 1013, 4714, 3126, 2140, 1012, 4012, 1013, 2484, 8490, 3501, 22025, 102], [101, 1052, 20959, 2229, 2327, 2753, 5855, 5049, 4620, 1997, 2257, 2230, 1012, 1012, 1012, 8299, 1024, 1013, 1013, 2978, 1012, 1048, 2100, 1013, 1038, 18195, 2100, 2480, 2692, 1001, 5855, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "\n",
      "[None, 0, 1, 2, 2, 2, 3, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, None]\n"
     ]
    }
   ],
   "source": [
    "examples = wnut['train'][:3]\n",
    "print(\"examples:\\n\" + str(examples) + \"\\n\")\n",
    "\n",
    "tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "print(\"tokenized_inputs:\\n\" + str(tokenized_inputs) + '\\n')\n",
    "\n",
    "# word_ids是一个方法，不指定batch_index至少加上括号啊！（不指定默认是0）\n",
    "print(tokenized_inputs.word_ids(batch_index=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "44c7e553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example:\n",
      "{'id': '0', 'tokens': ['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1030, 2703, 17122, 2009, 1005, 1055, 1996, 3193, 2013, 2073, 1045, 1005, 1049, 2542, 2005, 2048, 3134, 1012, 3400, 2110, 2311, 1027, 9686, 2497, 1012, 3492, 2919, 4040, 2182, 2197, 3944, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 进一步分词为subwords\n",
    "\n",
    "example = wnut[\"train\"][0]\n",
    "print(\"example:\\n\" + str(example) + \"\\n\")\n",
    "\n",
    "tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30781467",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"O\", \"B-BIAS\", \"I-BIAS\"] # 根据需要添加其他标签\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4458ff",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf22e101",
   "metadata": {},
   "source": [
    "# 二、训练Bias-NER模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d0cf63",
   "metadata": {},
   "source": [
    "## 1.数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e768cec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GXL\\.conda\\envs\\huggingface\\lib\\site-packages\\huggingface_hub\\repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "bias_conll = load_dataset(\"newsmediabias/BIAS-CONLL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f677eced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['Unnamed: 0', 'sentence_id', 'words', 'labels'],\n",
       "         num_rows: 50575\n",
       "     })\n",
       " }),\n",
       " {'Unnamed: 0': [0, 1, 2],\n",
       "  'sentence_id': [0, 0, 0],\n",
       "  'words': ['rachel_scanlan', 'thanks', 'for'],\n",
       "  'labels': ['O', 'O', 'O']})"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_conll, bias_conll[\"train\"][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "107799f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个样本已经处理完\n",
      "第1000个样本已经处理完\n",
      "第2000个样本已经处理完\n",
      "第3000个样本已经处理完\n",
      "第4000个样本已经处理完\n",
      "第5000个样本已经处理完\n",
      "第6000个样本已经处理完\n",
      "第7000个样本已经处理完\n",
      "第8000个样本已经处理完\n",
      "第9000个样本已经处理完\n",
      "第10000个样本已经处理完\n",
      "第11000个样本已经处理完\n",
      "第12000个样本已经处理完\n",
      "第13000个样本已经处理完\n",
      "第14000个样本已经处理完\n",
      "第15000个样本已经处理完\n",
      "第16000个样本已经处理完\n",
      "第17000个样本已经处理完\n",
      "第18000个样本已经处理完\n",
      "第19000个样本已经处理完\n",
      "第20000个样本已经处理完\n",
      "第21000个样本已经处理完\n",
      "第22000个样本已经处理完\n",
      "第23000个样本已经处理完\n",
      "第24000个样本已经处理完\n",
      "第25000个样本已经处理完\n",
      "第26000个样本已经处理完\n",
      "第27000个样本已经处理完\n",
      "第28000个样本已经处理完\n",
      "第29000个样本已经处理完\n",
      "第30000个样本已经处理完\n",
      "第31000个样本已经处理完\n",
      "第32000个样本已经处理完\n",
      "第33000个样本已经处理完\n",
      "第34000个样本已经处理完\n",
      "第35000个样本已经处理完\n",
      "第36000个样本已经处理完\n",
      "第37000个样本已经处理完\n",
      "第38000个样本已经处理完\n",
      "第39000个样本已经处理完\n",
      "第40000个样本已经处理完\n",
      "第41000个样本已经处理完\n",
      "第42000个样本已经处理完\n",
      "第43000个样本已经处理完\n",
      "第44000个样本已经处理完\n",
      "第45000个样本已经处理完\n",
      "第46000个样本已经处理完\n",
      "第47000个样本已经处理完\n",
      "第48000个样本已经处理完\n",
      "第49000个样本已经处理完\n",
      "第50000个样本已经处理完\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[160], line 34\u001b[0m\n\u001b[0;32m     30\u001b[0m grouped_train_dataset \u001b[38;5;241m=\u001b[39m group_words_labels(bias_conll[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# grouped_data = bias_conll[\"train\"].map(group_words_labels, batched=True, batch_size=25)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 将聚合后的数据转换为 Dataset 类型\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m grouped_train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_dict({\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m: grouped_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: grouped_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     37\u001b[0m })\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# 显示结果\u001b[39;00m\n\u001b[0;32m     40\u001b[0m grouped_train_dataset, grouped_train_dataset[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# 定义分组和聚合函数\n",
    "def group_words_labels(examples):\n",
    "    # 分组操作在这里是逐个执行的，所以我们需要先将它们累积在列表中\n",
    "    grouped_words = []\n",
    "    grouped_labels = []\n",
    "    previous_id = None\n",
    "    words = []\n",
    "    labels = []\n",
    "    for i in range(len(examples['sentence_id'])):\n",
    "        if previous_id is None or previous_id == examples['sentence_id'][i]:\n",
    "            # 如果是同一句子，累积单词和标签\n",
    "            words.append(examples['words'][i])\n",
    "            labels.append(examples['labels'][i])\n",
    "        else:\n",
    "            # 如果是新句子，保存前一句子的累积结果，并重新开始累积\n",
    "            grouped_words.append(words)\n",
    "            grouped_labels.append(labels)\n",
    "            words = [examples['words'][i]]\n",
    "            labels = [examples['labels'][i]]\n",
    "        previous_id = examples['sentence_id'][i]\n",
    "        if(i % 1000 == 0):\n",
    "            print(\"第\" + str(i) + \"个样本已经处理完\")\n",
    "    # 确保最后一句话也被添加\n",
    "    grouped_words.append(words)\n",
    "    grouped_labels.append(labels)\n",
    "    # 返回分组结果\n",
    "    return {'words': grouped_words, 'labels': grouped_labels}\n",
    "\n",
    "# 对训练集应用分组函数\n",
    "grouped_train_dataset = group_words_labels(bias_conll[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "71f8d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 映射\n",
    "\n",
    "label_list = ['O', 'B-BIAS', 'I-BIAS'] # 根据需要添加其他标签\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4bed59fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ba40ed266e44b1ac932fb8d8289a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': [['rachel_scanlan', 'thanks', 'for', 'following', 'your', 'website', 'looks', 'like', 'greek', 'to', 'me', 'and', 'i', 'couldn', 't', 'see', 'your', 'tweets'], ['Waiting', 'for', 'it', 'to', 'rain', 'for', 'our', 'golf', 'tournament', 'today']], 'labels': [[0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 2, 0, 0, 1, 0, 0, 0, 0], [1, 2, 0, 0, 0, 0, 0, 1, 2, 0]]}\n"
     ]
    }
   ],
   "source": [
    "# 将labels从类标签转化为digits\n",
    "\n",
    "# 转换标签到 ID 的函数\n",
    "def labels_to_ids(examples):\n",
    "    # 使用标签到 ID 的映射来转换每个标签\n",
    "    try:\n",
    "        examples['labels'] = [[label_to_id[label] for label in sentence_labels] for sentence_labels in examples['labels']]\n",
    "    except KeyError as e:\n",
    "        print(f\"找不到标签: {e}\")\n",
    "    return examples\n",
    "\n",
    "# 在 grouped_train_dataset 上应用这个函数\n",
    "grouped_train_dataset = grouped_train_dataset.map(labels_to_ids, batched=True)\n",
    "\n",
    "# 查看转换后的前两个样本\n",
    "print(grouped_train_dataset[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "aa1207e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['words', 'labels'],\n",
       "     num_rows: 2715\n",
       " }),\n",
       " {'words': [['rachel_scanlan',\n",
       "    'thanks',\n",
       "    'for',\n",
       "    'following',\n",
       "    'your',\n",
       "    'website',\n",
       "    'looks',\n",
       "    'like',\n",
       "    'greek',\n",
       "    'to',\n",
       "    'me',\n",
       "    'and',\n",
       "    'i',\n",
       "    'couldn',\n",
       "    't',\n",
       "    'see',\n",
       "    'your',\n",
       "    'tweets'],\n",
       "   ['Waiting',\n",
       "    'for',\n",
       "    'it',\n",
       "    'to',\n",
       "    'rain',\n",
       "    'for',\n",
       "    'our',\n",
       "    'golf',\n",
       "    'tournament',\n",
       "    'today']],\n",
       "  'labels': [[0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 2, 0, 0, 1, 0, 0, 0, 0],\n",
       "   [1, 2, 0, 0, 0, 0, 0, 1, 2, 0]]})"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# 将聚合后的数据转换为 Dataset 类型\n",
    "grouped_train_dataset = Dataset.from_dict({\n",
    "    'words': grouped_train_dataset['words'],\n",
    "    'labels': grouped_train_dataset['labels']\n",
    "})\n",
    "\n",
    "# 显示结果\n",
    "grouped_train_dataset, grouped_train_dataset[0: 2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d67638f",
   "metadata": {},
   "source": [
    "## 2.加载tokenizer,进一步处理数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e2ed27",
   "metadata": {},
   "source": [
    "### 2.1 加载tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "28da0073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前面已经加载过了，所以这里只是粘贴一下代码\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed20d62b",
   "metadata": {},
   "source": [
    "### 2.2 添加id列  & 修改words列和labels列名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "b4a20994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['tokens', 'ner_tags', 'id'],\n",
       "     num_rows: 2715\n",
       " }),\n",
       " {'tokens': [['rachel_scanlan',\n",
       "    'thanks',\n",
       "    'for',\n",
       "    'following',\n",
       "    'your',\n",
       "    'website',\n",
       "    'looks',\n",
       "    'like',\n",
       "    'greek',\n",
       "    'to',\n",
       "    'me',\n",
       "    'and',\n",
       "    'i',\n",
       "    'couldn',\n",
       "    't',\n",
       "    'see',\n",
       "    'your',\n",
       "    'tweets'],\n",
       "   ['Waiting',\n",
       "    'for',\n",
       "    'it',\n",
       "    'to',\n",
       "    'rain',\n",
       "    'for',\n",
       "    'our',\n",
       "    'golf',\n",
       "    'tournament',\n",
       "    'today']],\n",
       "  'ner_tags': [[0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 2, 0, 0, 1, 0, 0, 0, 0],\n",
       "   [1, 2, 0, 0, 0, 0, 0, 1, 2, 0]],\n",
       "  'id': ['0', '1']})"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_list = [str(i) for i in range(len(grouped_train_dataset))]\n",
    "\n",
    "# 必须添加id列\n",
    "# 因为map()方法进行分批时，需要用到id,如果没有就会报错\n",
    "# grouped_train_dataset = grouped_train_dataset.add_column(\"id\", id_list)\n",
    "\n",
    "\n",
    "# 给words和labels列改名\n",
    "# grouped_train_dataset = grouped_train_dataset.rename_column(\"labels\", \"ner_tags\")\n",
    "# grouped_train_dataset = grouped_train_dataset.rename_column(\"words\", \"tokens\")\n",
    "grouped_train_dataset, grouped_train_dataset[0:2] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f016e785",
   "metadata": {},
   "source": [
    "### 转换ner-tags的数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "184462e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9137842bd6745208f5f0abf3273c28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'id'],\n",
       "    num_rows: 2715\n",
       "})"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import ClassLabel, Sequence\n",
    "class_label = ClassLabel(names=['O', 'B-BIAS', 'I-BIAS'])\n",
    "\n",
    "new_features = grouped_train_dataset.features.copy()\n",
    "sequence_class_label = Sequence(feature=class_label, length=-1)\n",
    "new_features['ner_tags'] = sequence_class_label\n",
    "grouped_train_dataset_ = grouped_train_dataset.cast(new_features)\n",
    "grouped_train_dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "c5f8595b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-BIAS', 'I-BIAS'], id=None), length=-1, id=None),\n",
       " 'id': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_train_dataset_.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f514b62d",
   "metadata": {},
   "source": [
    "### 交换key的顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "f9dbf8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138e0036c2224f2eb09692d87e5623e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def reorder_features(example):\n",
    "    return {'id': example['id'], 'tokens': example['tokens'], 'ner_tags': example['ner_tags']}\n",
    "\n",
    "# 使用 map 函数应用这个转换，创建一个新的数据集\n",
    "reordered_dataset = grouped_train_dataset_.map(reorder_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "549a7b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf1e54e2fd24607a86e062a6d57f93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-BIAS', 'I-BIAS'], id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, Features, Value, Sequence, ClassLabel\n",
    "\n",
    "# 获取原始数据集的特征，并按新的顺序重新排列\n",
    "original_features = grouped_train_dataset_.features\n",
    "new_order_features = Features({\n",
    "    'id': original_features['id'],\n",
    "    'tokens': original_features['tokens'],\n",
    "    'ner_tags': original_features['ner_tags']\n",
    "})\n",
    "\n",
    "# 创建一个新的数据集，其中列的顺序是按照新的特征顺序排列的\n",
    "reordered_dataset = grouped_train_dataset_.cast(new_order_features)\n",
    "\n",
    "# 检查新数据集的特征以确认顺序\n",
    "print(reordered_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "4f1600dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'id': Value(dtype='string', id=None),\n",
       "  'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       "  'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-BIAS', 'I-BIAS'], id=None), length=-1, id=None)},\n",
       " {'id': Value(dtype='string', id=None),\n",
       "  'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       "  'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-corporation', 'I-corporation', 'B-creative-work', 'I-creative-work', 'B-group', 'I-group', 'B-location', 'I-location', 'B-person', 'I-person', 'B-product', 'I-product'], id=None), length=-1, id=None)})"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordered_dataset.features, wnut[\"validation\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a4a3b",
   "metadata": {},
   "source": [
    "### 2.3 进一步分词和labels对齐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "4d7d6ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "0d950ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2715"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reordered_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb5230e",
   "metadata": {},
   "source": [
    "### 🚩清除坏数据！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "572be1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '314',\n",
       " 'tokens': ['Only',\n",
       "  '3',\n",
       "  'concerts',\n",
       "  'quot',\n",
       "  'HITY',\n",
       "  None,\n",
       "  'CZASIE',\n",
       "  'quot',\n",
       "  'in',\n",
       "  'these',\n",
       "  'summer',\n",
       "  'holidays'],\n",
       " 'ner_tags': [1, 2, 1, 0, 1, 2, 2, 0, 0, 0, 1, 2]}"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordered_dataset[314]\n",
    "# 包含none元素，会导致使用tokenize_and_align_labels方式中的tokenizer的时候报错：\n",
    "# TypeError: PreTokenizedEncodeInput must be Union[PreTokenizedInputSequence, \n",
    "# Tuple[PreTokenizedInputSequence, PreTokenizedInputSequence]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "95b028cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a5712529e24fe99a216bf5b166a676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def remove_none_values(example):\n",
    "    # 获取tokens和ner_tags列表\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "    \n",
    "    # 使用列表推导式移除None值及其对应的ner_tags\n",
    "    new_tokens, new_ner_tags = zip(*[(token, tag) for token, tag in zip(tokens, ner_tags) if token is not None])\n",
    "    \n",
    "    # 更新example\n",
    "    example['tokens'] = list(new_tokens)\n",
    "    example['ner_tags'] = list(new_ner_tags)\n",
    "    return example\n",
    "\n",
    "# 应用这个函数到整个数据集\n",
    "cleaned_dataset = reordered_dataset.map(remove_none_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "ca2d8dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'ner_tags'],\n",
       "    num_rows: 2715\n",
       "})"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "18f24c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7df85abf444f1d9452ef37f9be4067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_dataset = cleaned_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "ac64c0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2715\n",
       "})"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae394f09",
   "metadata": {},
   "source": [
    "### 划分训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "effc56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = final_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# 获取训练集和验证集\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "e8ca975f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 2172\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 543\n",
       " }))"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d624175b",
   "metadata": {},
   "source": [
    "### 🚩上传数据集到hugging face库中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "b52bcb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0afa05702a4e0eb60b7407a11dc1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4832a0c244640d6a7b041be1b1e902b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7fa8253bace4746946d7e2e68b208f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c415f3226b46fd944e42f18ce7dfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46bf6e734307462485e524c58586e18a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "# 假设 cleaned_dataset 是您要上传的数据集\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset ,\n",
    "    \"validation\": val_dataset\n",
    "})\n",
    "\n",
    "# 推送数据集到您的 Hugging Face 存储库\n",
    "dataset_dict.push_to_hub(\"GXLooong/cleaned_BIAS_CONLL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1655c8b",
   "metadata": {},
   "source": [
    "## 3.加载model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "5370286d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1bbfdde129147f2bedb373511c57cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=3, id2label=id_to_label, label2id=label_to_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11e4dd0",
   "metadata": {},
   "source": [
    "## 4. 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4b514c",
   "metadata": {},
   "source": [
    "### 4.1 评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "bde47384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "e7e78200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "acda18d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[392], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbias_detection_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     16\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     24\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m<string>:112\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, xpu_backend)\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\huggingface\\lib\\site-packages\\transformers\\training_args.py:1372\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(version\u001b[38;5;241m.\u001b[39mparse(torch\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mbase_version) \u001b[38;5;241m==\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16:\n\u001b[0;32m   1367\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m-> 1372\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1373\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (get_xla_device_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[0;32m   1375\u001b[0m ):\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1377\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m     )\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1388\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[0;32m   1389\u001b[0m ):\n",
      "File \u001b[1;32m~\\.conda\\envs\\huggingface\\lib\\site-packages\\transformers\\training_args.py:1795\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1794\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 1795\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\huggingface\\lib\\site-packages\\transformers\\utils\\generic.py:54\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     52\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32m~\\.conda\\envs\\huggingface\\lib\\site-packages\\transformers\\training_args.py:1716\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   1715\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available(min_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.20.1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1716\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   1717\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1718\u001b[0m         )\n\u001b[0;32m   1719\u001b[0m     AcceleratorState\u001b[38;5;241m.\u001b[39m_reset_state(reset_partial_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bias_detection_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe89d901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

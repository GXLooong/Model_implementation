{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "209fa811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6da096d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7459f4ea0e94b7d8de8a67546da4a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 登陆不了\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8b9103d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'datasets' has no attribute 'utils' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      3\u001b[0m wnut \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwnut_17\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\huggingface\\lib\\site-packages\\datasets\\__init__.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfingerprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disable_caching, enable_caching, is_caching_enabled, set_caching_enabled\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetInfo, MetricInfo\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     get_dataset_config_info,\n\u001b[0;32m     33\u001b[0m     get_dataset_config_names,\n\u001b[0;32m     34\u001b[0m     get_dataset_infos,\n\u001b[0;32m     35\u001b[0m     get_dataset_split_names,\n\u001b[0;32m     36\u001b[0m     inspect_dataset,\n\u001b[0;32m     37\u001b[0m     inspect_metric,\n\u001b[0;32m     38\u001b[0m     list_datasets,\n\u001b[0;32m     39\u001b[0m     list_metrics,\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miterable_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IterableDataset\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, load_dataset_builder, load_from_disk, load_metric\n",
      "File \u001b[1;32m~\\.conda\\envs\\huggingface\\lib\\site-packages\\datasets\\inspect.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_download_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StreamingDownloadManager\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetInfo\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     dataset_module_factory,\n\u001b[0;32m     33\u001b[0m     get_dataset_builder_class,\n\u001b[0;32m     34\u001b[0m     import_main_class,\n\u001b[0;32m     35\u001b[0m     load_dataset_builder,\n\u001b[0;32m     36\u001b[0m     metric_module_factory,\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m relative_to_absolute_path\n",
      "File \u001b[1;32m~\\.conda\\envs\\huggingface\\lib\\site-packages\\datasets\\load.py:58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Metric\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaming\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m camelcase_to_snakecase, snakecase_to_camelcase\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpackaged_modules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     59\u001b[0m     _EXTENSION_TO_MODULE,\n\u001b[0;32m     60\u001b[0m     _MODULE_SUPPORTS_METADATA,\n\u001b[0;32m     61\u001b[0m     _MODULE_TO_EXTENSIONS,\n\u001b[0;32m     62\u001b[0m     _PACKAGED_DATASETS_MODULES,\n\u001b[0;32m     63\u001b[0m     _hash_python_lines,\n\u001b[0;32m     64\u001b[0m )\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msplits\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Split\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n",
      "File \u001b[1;32m~\\.conda\\envs\\huggingface\\lib\\site-packages\\datasets\\packaged_modules\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhashlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sha256\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m arrow\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudiofolder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audiofolder\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csv\n",
      "File \u001b[1;32m~\\.conda\\envs\\huggingface\\lib\\site-packages\\datasets\\packaged_modules\\arrow\\arrow.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m table_cast\n\u001b[1;32m---> 11\u001b[0m logger \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mArrowConfig\u001b[39;00m(datasets\u001b[38;5;241m.\u001b[39mBuilderConfig):\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"BuilderConfig for Arrow.\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'datasets' has no attribute 'utils' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wnut = load_dataset(\"wnut_17\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9c5ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wnut[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30781467",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"O\", \"B-BIAS\", \"I-BIAS\"] # 根据需要添加其他标签\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bda9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "from datasets import Dataset\n",
    "\n",
    "# 载入预训练的 tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 假设您的数据已经加载到了一个 Pandas DataFrame 中\n",
    "# 您需要将它转换为 Hugging Face 的 Dataset 对象\n",
    "data = ... # 从文件加载您的 DataFrame\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# 定义一个函数来编码数据\n",
    "def encode_example(example):\n",
    "    # 编码文本\n",
    "    encoding = tokenizer(example['words'], is_split_into_words=True, truncation=True, padding='max_length')\n",
    "    # 将标签转换为对应的索引\n",
    "    labels = [label_to_id[label] for label in example['labels']]\n",
    "    # 对标签使用和 token 相同的 padding 方法\n",
    "    labels += [label_to_id[\"O\"]] * (tokenizer.model_max_length - len(labels))\n",
    "    return {**encoding, 'labels': labels}\n",
    "\n",
    "# 应用函数对数据集进行编码\n",
    "encoded_dataset = dataset.map(encode_example, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5bc170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a96fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c923c6f",
   "metadata": {},
   "source": [
    "# 0.数据（用于调试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e23f2ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  torch.Size([16, 256])\n",
      "Y:  torch.Size([16, 256])\n"
     ]
    }
   ],
   "source": [
    "X = torch.load('X.tensor').to('cpu')\n",
    "Y = torch.load('Y.tensor').to('cpu')\n",
    "\n",
    "print(\"X: \", X.shape)\n",
    "print(\"Y: \", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9305a42a",
   "metadata": {},
   "source": [
    "# 1.GPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8aa093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    #  GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    #  怎么对vocab进行pad? 没有直观感受\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92757121",
   "metadata": {},
   "source": [
    "# 2.LayerNorm\n",
    "\n",
    "    ndim: 隐藏层维度，也就是num_hiddens\n",
    "    \n",
    "    weights和bias的形状与PyTorch官方实现的一样吗，都是normalized_shape\n",
    "    正确理解就是你之前的理解，对每个特征维度分布一对，而不是小冬瓜讲的那样\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0ebd2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "        pytorch中自带的LayerNorm类默认带有bias，并且不支持通过指定参数bias=False禁用偏置项，\n",
    "        所以这里自己定义一个LayerNorm层\n",
    "    \"\"\"\n",
    "    def __init__(self, ndim, bias):\n",
    "        # python2中为super(LayerNorm, self),即传入子类和对象实例作为参数\n",
    "        # 但是python3可以通过内部机制推断出正确的类层级关系并调用父类的初始化方法，直接使用这种简洁形式就可以了\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # 更底层的实现，比LayerNorm的实例化多传入weight和bias和eps\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326331e",
   "metadata": {},
   "source": [
    "# 3.SelfAttention\n",
    "\n",
    "    比transformer的attention多一个dropout操作（多了个resid_dropout）:\n",
    "        原来的attention只在masked attention matrix计算出来后进行dropout，\n",
    "        而这里对W_0的结果也进行了一次dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465ea22d",
   "metadata": {},
   "source": [
    "## 3.1 torch2.0以上自带的torch.nn.functional.scaled_dot_product方法的底层逻辑\n",
    "    \n",
    "    1.is_causal和attn_mask的关系\n",
    "            is_causal为True  和  attn_mask不为None   不能同时满足\n",
    "            从if is_causal:  assert attn_mask is None这里可以看出来\n",
    "            \n",
    "            虽然传入的是attn_mask，但是最后起作用的mask其实是attn_bias而不是attn_mask\n",
    "            is_causal和attn_mask都只是用于形成最终的attn_bias\n",
    "            \n",
    "            is_causal为True时（此时attn_mask必须为None）,attn_bias下三角（不包括对角线）为0，其它位置为-inf\n",
    "            is_causal为False,attn_mask不为None时：\n",
    "                attn_mask可能为torch.bool,也可能为0和-inf\n",
    "                不过最后也是行程0和-ing组成的attn_bias张量\n",
    "            （默认情况）is_causal为False并且attn_mask也为None，那么attn_bias全为0,也就是全都看得到（包括padding)\n",
    "            \n",
    "            最后把attn_bias和未经softmax的attention_weights相加，然后再softmax就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b10a4155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False],\n",
       "        [ True,  True, False, False],\n",
       "        [ True,  True,  True, False]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mask = torch.ones(3, 4, dtype=torch.bool).tril(diagonal=0)\n",
    "temp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d161817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 是这个逻辑，但是具体实现更高效，毕竟flashattention\n",
    "def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None) -> torch.Tensor:\n",
    "    # Efficient implementation equivalent to the following:\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "    attn_bias = torch.zeros(L, S, dtype=query.dtype)\n",
    "    if is_causal:\n",
    "        assert attn_mask is None\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        attn_bias.to(query.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    return attn_weight @ value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e01c4d",
   "metadata": {},
   "source": [
    "## 3.2借助scaled_dot_product实现selfattention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9c8b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 确保num_hiddens除以num_heads除得开\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # \n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias = config.bias) # 分qkv用\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias) # W_o\n",
    "        self.attn_dropout = nn.Dropout(config.dropout) # 给手动实现的attention用（slow attention)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout) # 给W_o用\n",
    "        self.dropout = config.dropout # 给flash attention用\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # hasattr方法不只可以检查类的实例中有没有属性，还可以检查模块中有没有属性/变量，方法，类等，用处非常广泛\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        # 如果torch>=2.0.0,使用自带的scaled_dot_product_attention方法\n",
    "        # 模型默认使用强制教学，自带方法可以通过设置is_causal为True来实现下三角掩码\n",
    "        # 而手动实现却没有下三角掩码，所以这里需要自己定义一个大小为max_len * max_len的下三角矩阵，以满足所有可能长度的强制教学\n",
    "        # 而且这个下三角矩阵是不需要更新的，所以就存在缓冲区即可\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # 缓冲区注册的bias，也是模型的一部分，并且与模型生命周期相同，强绑定\n",
    "            # 也可以通过causal_attention.bias.shape这种方式来访问\n",
    "            # 区别是不会被优化器作为可训练参数，常用于这种不会变的变量\n",
    "            # torch.tril的diagonal参数默认为0,表示包含对角线元素的下三角矩阵\n",
    "            self.register(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))).view(1, 1, config.block_size, config.block_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # batch_size, seq_len, num_hiddens\n",
    "        \n",
    "        # 获得形状为 B, num_heads, T, C/num_heads 的 QKV\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        \n",
    "        # dot_product_attention\n",
    "        if self.flash:\n",
    "            # pytorch自带实现\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p = self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # 手动实现\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "        y = y.transpose(1, 2).contiguouus().view(B, T, C)\n",
    "        \n",
    "        # W_o + dropout\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dc2bda",
   "metadata": {},
   "source": [
    "# 4. FFN(这里就叫做MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b79092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4*config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU() # 本质都是非线性变化，不必深究\n",
    "        self.c_proj = nn.Linear(4*config.n_embd, config.n_embd, bias = config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.gelu(self.c_fc(x))\n",
    "        return self.dropout(self.c_proj(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29533353",
   "metadata": {},
   "source": [
    "# 5.decoderBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6043e1f5",
   "metadata": {},
   "source": [
    "    与transformer中decoder的区别：\n",
    "         1.前者顺序为：\n",
    "            attention → add → layernorm → ffn → add → layernorm\n",
    "          后者顺序为：\n",
    "             layernorm → attention → add → layernorm → mlp → add\n",
    "         2.之前add&nom模块中有dropout操作，现在没有add&norm了，dropout并没有消失，而是转到了attention和mlp块中\n",
    "          仔细看下：\n",
    "              causalattention中对W_o的dropout\n",
    "              mlp中对c_proj的dropout\n",
    "          这在transformer中都是没有的，在gpt中的其实就是add&norm块中转移过去的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c701b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 没有dropout，dropout都被包含到了attn和mlp中\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc7d9c9",
   "metadata": {},
   "source": [
    "# 6.GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9c17f3",
   "metadata": {},
   "source": [
    "    1.对于 for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "         的理解：\n",
    "         这种初始化方式借鉴了GPT-2论文中所提出的建议，即对残差结构中最后的线性层（也称为分类器前的最后一层）\n",
    "         应用一个特殊的缩放初始化策略，旨在更好地控制模型训练初期的信号传播和梯度更新。\n",
    "    2.pos_encoding不再自己提前设定了，而是让模型自己学（非常奔放\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8365ab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd), # vocab embedding\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),  # pos embedding\n",
    "            drop = nn.Dropout(config.dropout), # 用于两个embedding操作完成并相加后，进行drop\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias)  # 全部block结束之后，再layernorm一下\n",
    "        ))\n",
    "        \n",
    "        # 输出头\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # weight typing，一种权重共享的操作，可能会引起警告，但是不耽误实际运行\n",
    "        # (lm_head): Linear(in_features=768, out_features=65, bias=False)\n",
    "        # Linear层的weight形状不是768 x 65！！而是65 x 768!!\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        \n",
    "        # 权重初始化\n",
    "        # 其实仔细看下，GPT中的可更新权重全部都是Linear和embedding,layernorm的在类里面已经初始化过了，不需要了\n",
    "        self.apply(self._init_weights)\n",
    "        # Module()类的属性named_parameters()方法可以返回一个 生成器生成所有可更新参数的name和parameter\n",
    "        for name, parameter in self.named_parameters():\n",
    "            if name.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(parameter, mean=0.0, std=0.02/math.sqrt(2*config.n_layer))\n",
    "        # 顺便输出参数数量\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6, ))\n",
    "        \n",
    "    def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eccf51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c9ec8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

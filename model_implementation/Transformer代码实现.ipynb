{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b40280",
   "metadata": {},
   "source": [
    "# 包引入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f0e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import math\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e27d93",
   "metadata": {},
   "source": [
    "# chapter 1 : Transformer Encoder的实现\n",
    "    \n",
    "    先实现EncoderBlock，\n",
    "    然后和 Embedding 以及 PositionalEncoding 组合称为Encoder\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188f5cdb",
   "metadata": {},
   "source": [
    "## 一、Encoder Block前的处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e42560a",
   "metadata": {},
   "source": [
    "### 1.embedding层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3a56c8",
   "metadata": {},
   "source": [
    "    输入的形状为 B x L, 输出形状为 B x L x num_hiddens\n",
    "    直接用pytorch自带的nn.Embedding()就可以\n",
    "    \n",
    "    不单独写代码了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28afe1fe",
   "metadata": {},
   "source": [
    "### 2.位置编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b187d05",
   "metadata": {},
   "source": [
    "#### 理解：\n",
    "        基于三角函数设计的位置编码是模仿数字二进制编码得来的，也就是：\n",
    "        二进制低位编码数字的变换频率是高于高位的\n",
    "        同时对于每一列，也随着行数的变化而变化\n",
    "        虽然对于人类理解起来不太直观，不过只要模型够强大，还是能学习到这种微妙的变化的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadc176a",
   "metadata": {},
   "source": [
    "#### 形状：\n",
    "    输入 X\n",
    "    输出 X + P \n",
    "    X和P的形状都是 B x L x num_hiddens\n",
    "    \n",
    "    但是这里因为需要有seq_len的max_len，又提前不知道B，所以init的时候P的形状为 1 x maxlen x num_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10743f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_hiddens, dropout, maxlen=10000): # 有默认值的参数一般放在最后，传参数的时候方便直接不写（如果在中间就不行了）\n",
    "        \"\"\"\n",
    "        num_hiddens和maxlen用于指定位置编码矩阵的形状,maxlen指定输入句子的最长长度，所以一开始就指定了\n",
    "        dropout就显而易见了，init的时候又要有drouput层\n",
    "        \n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # drouput\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 先定下P的形状\n",
    "        self.P = torch.zeros((1, maxlen, num_hiddens))\n",
    "        # 再填充三角函数值\n",
    "        # 前半部分形状为 maxlen x 1, 后半部分形状为 1 x num_hiddens/2\n",
    "        # 整体一除,形状为 maxlen x num_hiddens/2\n",
    "        X = torch.arange(maxlen, dtype=torch.float32).reshape(-1, 1)/ torch.pow(10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32)/num_hiddens)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # 🚩这里的to(X.device)操作有些迷惑，不知道到底什么时候需要传入device了\n",
    "        # 因为X与新创建的（凭空出现的）数据进行运算了，这种时候一般都得将新数据转到X的device上(还得考虑数据类型)\n",
    "        # 可以看看后面的数据传入，只要没有新创建的数据加进来，都不需要考虑device\n",
    "        # 新建的张量需要传入，但是标量好像不用传入，至少作为方法参数的标量不用传入！！\n",
    "        X = X + self.P[:, X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40a7e4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 1.],\n",
       "          [2., 3.]],\n",
       " \n",
       "         [[4., 5.],\n",
       "          [6., 7.]]]),\n",
       " tensor([[[0.9093, 0.5839],\n",
       "          [2.9093, 2.5839]],\n",
       " \n",
       "         [[4.9093, 4.5839],\n",
       "          [6.9093, 6.5839]]]),\n",
       " torch.Size([1, 10000, 2]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(8, dtype=torch.float32).reshape(2, 2, 2)\n",
    "pe = PositionalEncoding(2, 0)\n",
    "X, pe(X), pe.P.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ec914f",
   "metadata": {},
   "source": [
    "## 二、多头自注意力的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4aeeae",
   "metadata": {},
   "source": [
    "### 1.sequence_mask\n",
    "在序列中屏蔽不相关的项目"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c604a",
   "metadata": {},
   "source": [
    "    🚩 其实只用transformer的话，只是需要输入的valid_lens为一维的，二维的可能用于rnn，\n",
    "       所以sequence_mask中是有冗余代码的，有时间再整理下冗余代码，将sequence_mask直接并入masked_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50952851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00c4c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(X, valid_lens, value=0):\n",
    "    \"\"\"\n",
    "    X → 必须是一个二维张量，（B， L）/ (B*num_head, L)\n",
    "    valid_lens → 一维张量，表示每一个句子的有效长度\n",
    "    value → 用来填充多余位置的value\\\n",
    "    \"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    # 输出是一个bool形数组  形状 B x L\n",
    "    mask = torch.arange(maxlen, dtype=torch.float32, device=X.device)[None, :] < valid_lens[:, None] \n",
    "    # 用value填充不需要的地方\n",
    "    X[~mask] = value\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e661f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b261320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [4, 5, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证效果\n",
    "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "valid_lens = torch.tensor([1, 2]) # 必须是tensor， 否则不能使用方法中的升维操作\n",
    "\n",
    "sequence_mask(X, valid_lens, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d9a9f8",
   "metadata": {},
   "source": [
    "###  2.masked softmax\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "debbf1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked softmax\n",
    "\n",
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"\n",
    "    X → 三维 Batch_size(几个句子) L L （后两个维度表示一个attention matrix）\n",
    "    valid_lens → 一维（最常见的场景，一个batch里每个句子有自己各自的长度）Batch_size / \n",
    "                  二维（没见过，不理解，每个句子的每个单词的向量表示都有不同长度？） Batch_size x L(sequence_Len)\n",
    "    \"\"\"\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        # 保留形状恢复X\n",
    "        shape = X.shape\n",
    "        # 为了使用sequence_mask, X需要展成二维的 BL x L， valid_lens需要变成一维的BL\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # -1e6的自然指数就可以当作0了，同时X被展成BL x L\n",
    "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens, -1e6)\n",
    "        X = X.reshape(shape)\n",
    "        return nn.functional.softmax(X, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "679c0778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3207, 0.6793, 0.0000, 0.0000],\n",
       "         [0.6715, 0.3285, 0.0000, 0.0000],\n",
       "         [0.4910, 0.5090, 0.0000, 0.0000],\n",
       "         [0.3532, 0.6468, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.3831, 0.3992, 0.2177, 0.0000],\n",
       "         [0.2863, 0.3442, 0.3694, 0.0000],\n",
       "         [0.4003, 0.2702, 0.3295, 0.0000],\n",
       "         [0.3123, 0.3479, 0.3398, 0.0000]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证效果\n",
    "masked_softmax(torch.rand(2, 4, 4), torch.tensor([2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62619af0",
   "metadata": {},
   "source": [
    "### 3.DotProductAttention\n",
    "实现了self-attention的核心功能：\n",
    "\n",
    "attention_weights的计算和并将其和V相乘得到“裸”Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950ca8a8",
   "metadata": {},
   "source": [
    "🚩 注意：\n",
    "    \n",
    "    这里DotProduct的实现支持各种形状的attention_matrix，不光是 seq_len x seq_len\n",
    "    这就为后面decoder的预测输出提供了便利，因为：\n",
    "        预测第t+1个输出的情况下，\n",
    "        masked multi-head attention的keys和values是前t-1个输出结果的嵌入表示，形状（t-1， num_hiddens)\n",
    "        queries是第t个输出的嵌入表示，形状（1， num_hiddens)\n",
    "        这种情况下使用的DotProduct中的attention_matirx形状就是 （1， t-1），也是支持的！\n",
    "        \n",
    "        plus：\n",
    "        整个decoder走过一遍之后，输出的形状和第t个输出的形状一样，也是（1， num_hiddens）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf525d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        \"\"\"\n",
    "        输入的QKV均为 B x L x num_hidden\n",
    "        valid_lens因为是直接送入mask_softmax的，所以形状也可以为二维或者一维，\n",
    "        \"\"\"\n",
    "        d = queries.shape[-1]\n",
    "        # scores是没有经过softmax的注意力矩阵\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        # 暂退法防止过拟合, 至于为什么用在这个位置，记住之后，多多体会\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d489d169",
   "metadata": {},
   "source": [
    "### 4.Multi-Head self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021615ee",
   "metadata": {},
   "source": [
    "#### 4.1 不加multi-head的self attention\n",
    "如果不涉及多头，那么self-attention有以下流程：\n",
    "\n",
    "> 输入Input(B, L, num_hidden)乘以 Wq Wk Wv(num_hidden, num_hidden)变成 Q K V，\n",
    "\n",
    "> 然后使用DotProduct,输入QKV，输出O\n",
    "\n",
    "> O乘以W_o,得到Output\n",
    "\n",
    "但是我们要实现的是 Multi-Head-self attention。\n",
    "\n",
    "#### 4.2 如何加上multi-head\n",
    "#####  首先分析多头自注意力的输入和输出形状：\n",
    "    因为输出要concat，所以输出形状肯定是 (Batch_size, seq_len, num_heads, 隐藏层维度)\n",
    "    \n",
    "    但是计算注意力的话，seq_len和隐藏层两个维度肯定是要挨着的，又因为要并行计算，所以num_heads维度要往上调整\n",
    "    那么输入形状就应该是 (Batch_size, num_heads, seq_len, num_hiddens或者num_hiddens/num_heads)\n",
    "    最后不确定是num_hiddens/num_heads还是num_hiddens,不过李沐用的这个，就跟着用吧，gpt用的好像也是这个\n",
    "    \n",
    "##### DotProduct不支持四维输入：\n",
    "    DotProduct输入的QKV的形状是三维的B x L x num_hidden，而多头自注意力的输入为四维 (Batch_size, num_heads, seq_len, num_hiddens/num_heads)\n",
    "\n",
    "    怎么办呢？\n",
    "    \n",
    "    将num_heads先并到Batch_size维度里面（valid_lens也得跟着进行插值操作，以保证对的上），这些就可以直接用DotProduct了\n",
    "    之后再将DotProduct的输出分解为四维的 (Batch_size, num_heads, seq_len, 隐藏层维度)\n",
    "    \n",
    "    当然，上面对于输入维度进行调整的想法都只是直觉，这样利用DotProduct实现的是否最终是我们要的多头并行计算呢？这还得仔细想想才能明白。\n",
    "    画画输入的形状图就知道了，懒得讲了！\n",
    "    \n",
    "    搞定！\n",
    "    \n",
    "#### 4.2 multi-head self-attention的实现流程：\n",
    "> 输入Input(B, L, num_hidden)乘以 Wq Wk Wv(num_hidden, num_hidden)变成 Q K V，\n",
    "\n",
    "> 将QKV均转化为三维形状 (Batch_size* num_heads, seq_len, num_hiddens/num_heads) 🚩transpose_qkv方法\n",
    "\n",
    "> 将转化后的QKV输入DotProductAttention\n",
    "\n",
    "> 将DotProductAttention的输出转化为四维形状 (Batch_size, num_heads, seq_len, num_hiddens/num_heads) ，然后concat得到三维输出O (B, seq_len, num_hiddens)  🚩transpose_output方法  (不将W_o并入是因为还要传入W_o参数，麻烦，transopose方法中进行的全是不需要W的操作！num_heads不得不传入了）\n",
    "\n",
    "> O乘以W_o,得到Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b4050c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先实现两个transpose方法\n",
    "# 两个方法都无法从X中分辨出num_heads,所以都要传入\n",
    "\n",
    "def transpose_qkv(X, num_heads):\n",
    "    # 分割num_hiddens\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "    # 交换维度\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    # 合并batch_size和num_heads维度\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "def transpose_output(X, num_heads):\n",
    "    # 分割batch\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "    # 交换维度\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    # concat\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa94c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现多头自注意力机制\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # 因为初始化时需要定义网络结构，包括W，所以需要size和num_hiddens,包括DotProductAttention，它需要传入dropout参数，所以这里也要传入\n",
    "    # 所以为什么其它的功能要用方法实现，而这里的DotProductAttention要用class实现的，就是因为其中有网络结构！\n",
    "    # 并且，多头自注意力，有几个头总不能在forward的时候再传入吧，直接使用模型的人哪里知道你有几个头\n",
    "    # 🚩三个size可以直接用num_hiddens\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        # 保留，留着forward方法用。为什么不在forward的时候传入？？方法同上\n",
    "        self.num_heads = num_heads\n",
    "        # dropout和self_attention用的方法\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        # 四个可学习的W，因为使用的是DotProduct，所以只有这四个可学习的参数\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\n",
    "        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        \"\"\"\n",
    "            queries, keys，values 必须分开传入：\n",
    "                可能看到编码器中三者都相同，\n",
    "                但是在解码器中也有Multi-head attention,\n",
    "                它的三个传入就不同\n",
    "        \"\"\"\n",
    "        # 转化成DotProduct可以处理的形状\n",
    "        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n",
    "        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "        values = transpose_qkv(self.W_v(values), self.num_heads)\n",
    "        # valid_lens形状改变\n",
    "        if valid_lens is not None:\n",
    "            # 不管一维还是二维，都在dim0复制  （目前只能理解一维的）\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0)\n",
    "        # DotProduct处理\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        # 直接得到concat后的output(当然前面还有其它操作)\n",
    "        output_concat = transpose_output(output, self.num_heads)\n",
    "        # 乘以W_o\n",
    "        return self.W_o(output_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5c2db5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (attention): DotProductAttention(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (W_q): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_k): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_v): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,\n",
    "                               num_hiddens, num_heads, 0.5)\n",
    "attention.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "185a886a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, num_queries = 2, 4\n",
    "num_kvpairs, valid_lens =  6, torch.tensor([3, 2])\n",
    "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
    "Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "attention(X, Y, Y, valid_lens).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3e380f",
   "metadata": {},
   "source": [
    "🚩🚩 有时间一定把这个MultiHeadAttention改成纯MultiHeadSelfAttention!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce75b6f8",
   "metadata": {},
   "source": [
    "🚩 那gpt这些模型中的单次最大输入token是哪里规定的呢？？\n",
    "\n",
    "上面的方法好像除了Positional Encoding传入了maxlen,其它都没有传入参数/有相关限制， 难道在tokenizer中规定的?先继续往后看吧。\n",
    "   哈哈哈哈果然，刚问了gpt,说gpt2模型的最大token数主要由Positional Encoding限制。\n",
    "   当然，这只是技术上的限制，本质的原因还是transformer模型的计算复杂度与输入token的数量n成平方，O(n^2)，n设置太大的话时间和内存都顶不住。\n",
    "   当我们想对一个模型进行训练或者推理的时候，都会指定最小显存。如果max_len不指定的话，那吃的显存可能无限大，所以必须得规定一个max_token,最小显存可能就是对应的max_token的情况下使用的显存。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a49413a",
   "metadata": {},
   "source": [
    "## 三、EncoderBlock中的其它模块的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac60b7ee",
   "metadata": {},
   "source": [
    "    一个EncoderBlock的组成其实主要是\n",
    "        一个Multi-head self attention + 一个全连接（将num_hiddens映射到num_hiddens）\n",
    "    这个全连接层不叫全连接，而是叫PositionwiseFFN，为什么呢？\n",
    "        一般全连接输入的形状为 batch_size x input_dim,输出形状为 batch_size x output_dim\n",
    "        但是MultiHeadAttention的输出维度为 Batch_size x Seq_len x num_hiddens\n",
    "        就需要把seq_len集成到第0维或者第二维，但是如果继承到第二维，就要求seq_len是不可变的,\n",
    "        所以只能把seq_len集成到了batch_size的维度，效果上就是对每句话的每个词的表示都做个一次FCN\n",
    "        \n",
    "        不过上面说的这些直接用nn.Linear就可以实现，因为它的输入可以是三维的，但它只看最后一维！\n",
    "        \n",
    "        只是这里FCN不叫FCN，而是FFN，因为它有两个全连接和一个relu\n",
    "        PositionWise中的position就是指句子中的每一个单词\n",
    "        \n",
    "        \n",
    "    EncoderBlock又采用了残差网络的思想，得有add块，同时在add块中集成上normalization\n",
    "    (因为seq_len维度相当于Channel维度，但是channel固定，seq_len却不是固定的，所以只能用LayerNorm)\n",
    "    \n",
    "    \n",
    "    所以最后EncoderBlock结构就是：\n",
    "        MultiHeadAttention → Add&norm → PositionwiseFFN → Add&norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdf5548",
   "metadata": {},
   "source": [
    "### 1. PositionwiseFFN\n",
    "\n",
    "基于位置的前馈网络\n",
    "    \n",
    "    输入X形状 → B x Seq_len x num_hiddens\n",
    "    \n",
    "    输出形状  → 相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "234697b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_output, **kwargs):\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        # relu和dropout一样，会作用域每一个元素，不管多少维\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_output)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d2f51",
   "metadata": {},
   "source": [
    "### 2.Add & norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80cd9515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "    def forward(self, X, Y):\n",
    "        # Y是经过上一个模块处理得到的，X是未经处理的\n",
    "        # AddNorm里有dropout,位置编码里也有dropout,多头自注意力中也有dropout,只有FFN中没有，好！\n",
    "        return self.ln(self.dropout(Y) + X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e5024b",
   "metadata": {},
   "source": [
    "## 四、EncoderBlock的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1096427",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    参数解释：\n",
    "    三个size用于初始化multi-head attention\n",
    "    三个ffn用于初始化ffn\n",
    "    num_hiddens, num_heads就不用说了\n",
    "    dropout好多地方用得到\n",
    "    norm_shape有点意料之外，这样理解：\n",
    "        你想当然认为应该是[seq_len, num_hiddens],觉得这样只传入一个seq_len就好了。\n",
    "        但是不排除有人就指向以一个词语的嵌入表示作为一片进行性layernorm，这样输入就只是一个[num_hiddens]了\n",
    "        所以norm_shape需要通过形状告诉addnorm层我想进行的到底是什么。\n",
    "        🚩 但是啊但是，seq_len怎么会在初始化的时候传入呢？？\n",
    "            如果传入了[seq_len，num_hiddens], 那么seq_len要怎么指定呢？肯定不能少于maxlen,大概就是maxlen，\n",
    "            这样的话，Encoder的每次输入的句子都必须是maxlen了，这是对计算资源的一种极大浪费啊\n",
    "\n",
    "            所以可以看到后面的norm_shape都只是[num_hiddens], 可能就是考虑到这种浪费吧\n",
    "\n",
    "            那么不就注定了，Encoder中的layernorm的normalized_shape只能是它了吗？？\n",
    "\n",
    "            所以还传入norm_shape干嘛，直接用num_hiddens不就可以了？？\n",
    "            难道是为了保持灵活性？\n",
    "            不懂，全部看完之后再说吧。\n",
    "            \n",
    "            结论：应该是代码冗余了，传入的norm_shape只能是num_hiddens\n",
    "        \"\"\"\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "                 dropout, use_bias=False, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        self.attention = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "    def forward(self, X, valid_lens):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
    "        return self.addnorm2(Y, self.ffn(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e13683e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((2, 100, 24))\n",
    "valid_lens = torch.tensor([3, 2])\n",
    "encoder_blk = EncoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5)\n",
    "encoder_blk.eval()\n",
    "encoder_blk(X, valid_lens).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d810d",
   "metadata": {},
   "source": [
    "## 五、TransformerEncoder的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ed3f577",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(d2l.Encoder):\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "                 num_layers, dropout, use_bias=False, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        # 1.记录作用,当我之后要组织数据输入的时候，知道数据要嵌入的维度\n",
    "        # 2.后面embedding后缩放用得到\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i),\n",
    "                                EncoderBlock(key_size, query_size, value_size, num_hiddens,\n",
    "                                            norm_shape, ffn_num_input, ffn_num_hiddens,\n",
    "                                            num_heads, dropout, use_bias))\n",
    "    def forward(self, X, valid_lens, *args):\n",
    "        # 因为embedding后的特征嵌入太小了，都小于1，pos_encoding添加的三角函数值在-1，1范围内\n",
    "        # 所以要乘一个大数，突出下embedding信息\n",
    "        # 在DotProduct那里除以 sqrt(num_hiddens)是防止num_hiddens过大，点击结果过大\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "        \n",
    "        # 保存num_layers个attention_matrix\n",
    "        self.attention_weights = [None] * len(self.blks)\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, valid_lens)\n",
    "            # 第一个attention是MultiHeadAttention对象，第二个是DotProductAttention对象\n",
    "            self.attention_weights[i] = blk.attention.attention.attention_weights\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89344855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = TransformerEncoder(\n",
    "    200, 24, 24, 24, 24, [100, 24], 24, 48, 8, 2, 0.5)\n",
    "encoder.eval()\n",
    "encoder(torch.ones((2, 100), dtype=torch.long), valid_lens).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a758538b",
   "metadata": {},
   "source": [
    "# Chapter2 : TransformerDocoder的实现\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2116fa6",
   "metadata": {},
   "source": [
    "## 一、重要思考"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a68266",
   "metadata": {},
   "source": [
    "### 1.两个自注意力 & 预测的流程\n",
    "    DecoderBlock里有两个看似不一样的自注意力块：\n",
    "        Masked Multi-Head Attention 和 encoder-decoder/cross multi-head attention\n",
    "    其实这两个块并不用单独实现，还是用之前的Multi-head Attention就可以，改变输入就可以！\n",
    "    毕竟类定义的时候并没有传入seq_len，传入seq_len不同的QKV只是Attention_matrix形状不一样罢了(seq_q x seq_k)，\n",
    "    只要保持B和num_hiddens对的上，keys和values形状一样，就可以了。\n",
    "    \n",
    "    而且还有一点要意识到：Multi-Head Attention的输出形状是和query的形状一样的。\n",
    "    \n",
    "    cross输入的keys和values是encoder的输出 B x seq_len x num_hiddes, valid_len就是encoder输入的encoder，形状B\n",
    "    masked输入的keys和values比较复杂，要分预测和训练两种情况：\n",
    "        \n",
    "        预测情况下：\n",
    "        当预测第t+1个词的时候，用第t个词的嵌入表示做query,用前t个词的嵌入做keys和values\n",
    "        decoderblock总共有num_layers个，索引为i\n",
    "            i=1，keys和values为前t个词的嵌入表示 形状 t x num_hiddens\n",
    "            i!=1, keys和values就不是经过embedding的嵌入了，而是这些嵌入经过前i-1个块处理过后的中间状态，形状也是（t, num_hiddens）\n",
    "                但是这中间状态不是直接一下得到的，而是\n",
    "                预测前t-1个词的过程中保存在state的中间状态（t-1, num_hiddens）\n",
    "                concat上第t个预测经之前的decoders处理过，传到第i个block的中间状态(1， num_hiddens)\n",
    "                conncat之后顺便再保存到state中，预测第t+2个输出的时候用\n",
    "                （所以意识到，decoder模型的输出是保存在state中的）\n",
    "                state中保存的key_values全部都是X拼接而来的！\n",
    "                \n",
    "        虽然i=1和i!=1输入的“意义”有所不同，但是在decoderblock的实现中并不会有不同，都是合并输入的query和state,\n",
    "        然后再保存到state中，同时当作masked attention的keys和values。\n",
    "        \n",
    "        训练情况下：\n",
    "        如果按照预测的思路，state中保存的东西和对state的操作都不变，只是每次输出的X或query变成目标序列的每一个单词的query就可以了\n",
    "        然后把每一次的预测出的单词分布与目标单词分布做个损失反向传播然后更新就可以。\n",
    "        但是但是但是但是但是----\n",
    "        这样训练起来就太慢了，能不能所有单词的预测一起进行呢？的确有办法，下面就讲解这种方法。\n",
    "\n",
    "### 2.训练的并行实现\n",
    "    仔细思考思考之前的masked softmax挡住的是谁？\n",
    "    \n",
    "    挡住的是key_value的padding,而不是query的，因为attention_matrix上是横向置零，也就是第1维，\n",
    "    其实第0维也就是最后几行，是query的padding token的嵌入表示与keys的点积结果\n",
    "        （其实搞不懂为啥不把query的padding token也mask掉，maybe是因为只有刚输入时候表示padding token，\n",
    "         之后seq_len维度都是混乱的特征而不特指某一个词的嵌入了??)不过这不是重点。\n",
    "     也就是说decoderblock中两个attention的valid_len，cross的应该是encoder中用的，\n",
    "     而mask应该跟作为key_value的前t个预测的输出/目标输出的前t个词汇的嵌入表示 （B， t, num_hiddens）有关\n",
    "     \n",
    "     又因为decoder的输出跟输入维度相同，所以我们如果想一次性预测预测 目标词汇嵌入(B, seq_len, num_hiddens)的输出（B， seq, num_hiddens），\n",
    "     只需要也把它丢入网络就可以了，但是如果像encoder那样设置valid_len形状为[B],那么目标词汇嵌入就全部都被看到了！\n",
    "     所以我们需要改进masked multi-head attention的valid_len，以实现所谓的“masked”\n",
    "     \n",
    "     怎么改进呢？太难想了，自己想是不可能的，直接理解吧。\n",
    "     方法是：\n",
    "         传入X 形状(B, seq_len, num_hiddens), 将其同时作为QKV，但是valid_len形状不再是[B],而是[B, seq_len]，\n",
    "         具体每一行都是torch.arange(1, seq_len+1),重复B行\n",
    "     先不考虑batch_size, 我们假设训练时输入只有一个句子，\n",
    "         着眼attention_matrix，它的第t行其实就表示作为query的X的第t行对应的输出要看key_value句子的前几个单词，\n",
    "         根据任务，第t行的输出其实就是看到前t个单词后对t+1个单词的预测\n",
    "         所以我们让第t行看到前t个单词就好了，\n",
    "         原来valid_len只是一个数字的情况下，所有行都看前valid_len个单词，现在我们每一行都不一样了，第t行看前t个\n",
    "         所以batch_size是1的情况下，valid_len不能只是一个数字了，而是seq_queries了，也就是seq_len\n",
    "     那么当batch_size是B的情况下，valid_len的形状就是[B, seq_len]了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313232a6",
   "metadata": {},
   "source": [
    "## 二、Transformer DecoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee76078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\" 解码器的第i个块\"\"\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_input, ffn_num_hiddens, \n",
    "                 num_heads, dropout, i, **kwargs):\n",
    "        super(DecoderBlock, self).__init__(**kwargs)\n",
    "        # 编号\n",
    "        self.i = i\n",
    "        # 主要组成\n",
    "        self.attention1 = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.attention2 = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm3 = AddNorm(norm_shape, dropout)\n",
    "        \n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
    "        \n",
    "        # 第一次传入，训练情况下X形状直接为(B, seq_len, num_hiddens),预测情况下，X为start token的嵌入表示（b, 1， num_hiddens）\n",
    "        if state[2][self.i] is None:\n",
    "            key_values = X\n",
    "        # 第一次之后的传入，只有预测才会执行这一步\n",
    "        else:\n",
    "            key_values = torch.cat((state[2][self.i], X), axis=1)\n",
    "        # 更新state，用于下一个词的预测(只有预测的时候才会这样，训练的时候每个decodeblock处理一次就结束了)\n",
    "        # 全部预测结束后，state[2]中保存的就是所有的预测结果的嵌入表示(B, 不一定多长， num_hiddens)\n",
    "        state[2][self.i] = key_values\n",
    "        \n",
    "        # training是所有Module子类都会有的属性，通过model.train()设置为true, model.eval()设置为false\n",
    "        if self.training:\n",
    "            batch_size, num_steps, _ = X.shape\n",
    "            dec_valid_lens = torch.arange(1, num_steps + 1, device=X.device).repeat(batch_size, 1)\n",
    "        else:\n",
    "            # eval的时候key——values都是由之前的处理结果拼接出来的，不存在padding\n",
    "            dec_valid_lens = None\n",
    "        \n",
    "        # 处理流程\n",
    "        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)\n",
    "        Y = self.addnorm1(X, X2) #一定记住，第一个参数是未经处理的X\n",
    "        Y2 = self.attention2(X, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "        Z = self.addnorm2(Y, Y2)\n",
    "        return self.addnorm3(Z, self.ffn(Z)), state\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "295bafb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_blk = DecoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5, 0)\n",
    "decoder_blk.eval()\n",
    "X = torch.ones((2, 100, 24))\n",
    "state = [encoder_blk(X, valid_lens), valid_lens, [None]]\n",
    "decoder_blk(X, state)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a744fe0",
   "metadata": {},
   "source": [
    "## 三、TransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ceed7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(d2l.AttentionDecoder):\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_input, ffn_num_hiddens,\n",
    "                 num_heads, num_layers, dropout, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        # forward方法里肯定都用得到\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers # 构建state用得到\n",
    "        \n",
    "        # 层\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i),\n",
    "                                 DecoderBlock(key_size, query_size, value_size, num_hiddens,\n",
    "                                              norm_shape, ffn_num_input, ffn_num_hiddens,\n",
    "                                              num_heads, dropout, i))\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "        \n",
    "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
    "        return [enc_outputs, enc_valid_lens, [None] * self.num_layers]\n",
    "    \n",
    "    def forward(self, X, state):\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "        \n",
    "        # 保存cross和mask注意力权重\n",
    "        self._attention_weights = [[None] * len(self.blks) for _ in range(2)]\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X ,state = blk(X, state)\n",
    "            # masked\n",
    "            self._attention_weights[0][i] = blk.attention1.attention.attention_weights\n",
    "            # cross\n",
    "            self._attention_weights[1][i] = blk.attention2.attention.attention_weights\n",
    "        # 输出的第一项是最低维为vocab_size的概率\n",
    "        # 如果是训练，那么形状为(B, seq_len, vocab_size),如果是eval，那么形状为（B， 1， vocab_size）\n",
    "        # state用于预测下一个词的输出，只有在eval的时候才会有后续作用\n",
    "        # 如果是训练，那么state装的全都是X（B， seq_len， num_hiddens）及其经decoderblock处理后的相同形状的中间输出\n",
    "        return self.dense(X), state \n",
    "    \n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827026d4",
   "metadata": {},
   "source": [
    "# Chapter3 整合成Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c99e7",
   "metadata": {},
   "source": [
    "## 一、EncoderDecoder类\n",
    "    定义了Encoder和Decoder之间的数据流\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f19e43be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    # 这个args在这里是valid_lens\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ea8a0f",
   "metadata": {},
   "source": [
    "## 二、使用EncoderDecoder类集成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9a2096bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(\n",
    "    len(src_vocab), key_size, query_size, value_size, num_hiddens,\n",
    "    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "    num_layers, dropout)\n",
    "decoder = TransformerDecoder(\n",
    "    len(tgt_vocab), key_size, query_size, value_size, num_hiddens,\n",
    "    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "    num_layers, dropout)\n",
    "net = d2l.EncoderDecoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c7b06e",
   "metadata": {},
   "source": [
    "# Chapter4  训练、测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5c6d8a",
   "metadata": {},
   "source": [
    "## 一、训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6d20895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.1, 64, 10\n",
    "lr, num_epochs, device = 0.005, 200, d2l.try_gpu()\n",
    "ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4\n",
    "key_size, query_size, value_size = 32, 32, 32\n",
    "norm_shape = [32]\n",
    "\n",
    "# 加载数据\n",
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4296211f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.032, 4130.5 tokens/sec on cpu\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"262.1875pt\" height=\"183.35625pt\" viewBox=\"0 0 262.1875 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-12-08T23:43:50.712520</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 262.1875 183.35625 \n",
       "L 262.1875 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "L 50.14375 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 91.259539 145.8 \n",
       "L 91.259539 7.2 \n",
       "\" clip-path=\"url(#pdbe6dbbb73)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"mdf84c2e68a\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mdf84c2e68a\" x=\"91.259539\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(84.897039 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 142.654276 145.8 \n",
       "L 142.654276 7.2 \n",
       "\" clip-path=\"url(#pdbe6dbbb73)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mdf84c2e68a\" x=\"142.654276\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(133.110526 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 194.049013 145.8 \n",
       "L 194.049013 7.2 \n",
       "\" clip-path=\"url(#pdbe6dbbb73)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mdf84c2e68a\" x=\"194.049013\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 150 -->\n",
       "      <g transform=\"translate(184.505263 160.398438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "\" clip-path=\"url(#pdbe6dbbb73)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mdf84c2e68a\" x=\"245.44375\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(235.9 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_5\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(132.565625 174.076563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 50.14375 124.528179 \n",
       "L 245.44375 124.528179 \n",
       "\" clip-path=\"url(#pdbe6dbbb73)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <defs>\n",
       "       <path id=\"mb7865eb4cd\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb7865eb4cd\" x=\"50.14375\" y=\"124.528179\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0.05 -->\n",
       "      <g transform=\"translate(20.878125 128.327398) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 50.14375 83.663355 \n",
       "L 245.44375 83.663355 \n",
       "\" clip-path=\"url(#pdbe6dbbb73)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb7865eb4cd\" x=\"50.14375\" y=\"83.663355\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(20.878125 87.462574) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 50.14375 42.79853 \n",
       "L 245.44375 42.79853 \n",
       "\" clip-path=\"url(#pdbe6dbbb73)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb7865eb4cd\" x=\"50.14375\" y=\"42.79853\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.15 -->\n",
       "      <g transform=\"translate(20.878125 46.597749) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_9\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(14.798437 86.157813) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_15\">\n",
       "    <path d=\"M 50.14375 13.5 \n",
       "L 60.422697 49.293442 \n",
       "L 70.701645 77.633584 \n",
       "L 80.980592 94.816536 \n",
       "L 91.259539 106.315435 \n",
       "L 101.538487 113.963384 \n",
       "L 111.817434 121.159851 \n",
       "L 122.096382 126.88739 \n",
       "L 132.375329 126.888503 \n",
       "L 142.654276 129.515629 \n",
       "L 152.933224 131.564273 \n",
       "L 163.212171 132.929645 \n",
       "L 173.491118 132.990309 \n",
       "L 183.770066 135.072465 \n",
       "L 194.049013 136.159423 \n",
       "L 204.327961 136.772461 \n",
       "L 214.606908 136.579507 \n",
       "L 224.885855 138.050745 \n",
       "L 235.164803 139.403298 \n",
       "L 245.44375 139.5 \n",
       "\" clip-path=\"url(#pdbe6dbbb73)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 50.14375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 245.44375 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 50.14375 7.2 \n",
       "L 245.44375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pdbe6dbbb73\">\n",
       "   <rect x=\"50.14375\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 直接调用训练方法\n",
    "d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5993172d",
   "metadata": {},
   "source": [
    "## 二、测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cbeb9a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "917c439c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . => va ! bleu1.000\n",
      "i lost . => j'ai perdu . bleu1.000\n",
      "he's calm . => il est calme . bleu1.000\n",
      "i'm home . => je suis chez moi . bleu1.000\n"
     ]
    }
   ],
   "source": [
    "for eng, fra in zip(engs, fras):\n",
    "    translation, dec_attention_weights_seq = d2l.predict_seq2seq(net, eng, src_vocab, tgt_vocab, num_steps, device, True)\n",
    "    print(f'{eng} => {translation}', f'bleu{d2l.bleu(translation, fra, k=2):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3998dc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
